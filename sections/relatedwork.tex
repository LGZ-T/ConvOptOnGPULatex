\section{Related Work}
Numerous efforts have been dedicated to optimizing convolution operations. As previously mentioned, GEMM-, FFT- and
Winograd-based convolutions are broadly adopted convolution algorithms.

GEMM-based convolution is the first attempt to optimize convolution. Chellapilla et al. \cite{Chellapilla2006High} developed an unrolling
convolution algorithm  called the im2col convolution algorithm. 
%They lowered the convolutions into a matrix multiplication, which is highly optimized on GPUs. Despite some duplicate input elements involved in the computation, the performance gain of this algorithm is impressive.
Abdelfattah et al. \cite{abdelfattah2019fast} use a simple pruning strategy to search for the optimal tiling size. However, their method is
inadequate for depthwise separable convolution because they ignore SM utilization and arithmetic intensity when searching for the optimal
tiling size. Our approach avoids this problem by dynamically adjusting the tiling size.

A wide range of techniques on auto-tuning GEMM kernels have been proposed. Among these, the FFT- and Winograd-based convolutions are the
dominating methods because they can reduce computational complexity and improve convolution performance. Mathieu et al.
\cite{mathieu2013fast} proposed an FFT-based convolution to compute convolutions as pointwise products in the Fourier domain and reuse the
transformed input data, which significantly reduces the complexity of the convolution. However, FFT-based convolution is more suitable for
large filters than for small ones. Because padding the filters to the same size as the input data is necessary, and  the latter (e.g., $3
\times 3$ filters) needs more memory than the former. Lavin et al. \cite{lavin2016fast} used Winograd's minimal filtering algorithm to
accelerate the convolution on GPU. This algorithm can reduce the arithmetic complexity of convolution by up to four times compared with
direct convolution. However, Winograd's algorithm is only suitable for small filters due to its numerical instability. Zhen et al.
\cite{Zhen2018Optimizing} extended Winograd's algorithm to support any filter size. However, the traditional and extended Winograd-based
algorithms need to transform the input and filter before performing matrix multiplication, and both require more operations than the FFT
algorithm.

Transforming the input and filter before performing matrix multiplication incurs a large memory overhead, which can outweigh the
performance gains obtained through lowering the computational complexity. Therefore, recent studies have looked into minimizing the memory
overhead of the transformation phases. Cho et al. \cite{cho2017mec} reduced the memory overhead of GEMM-based convolutions using a compact
lowering scheme to reduce the redundancy in the lowered matrix and then performed multiple small matrix multiplications in parallel.
However, this algorithm still needs to transform the input and filter tensors into lowered matrixes to compute the convolution. Iandola et
al. \cite{Iandola2014Communication} reduced memory communication of 2D convolutions on GPU. They also prefetched the image regions to the
registers. While their method uses fewer threads, each thread operates on a larger number of data items. As a result, their method does not
reduce the number of global memory transactions. Unlike \cite{Iandola2014Communication}, our approach promotes register use and can
significantly reduce the number of memory accesses.

The work presented in \cite{oyama2018accelerating} splits larger batches into smaller batch sizes to mitigate computation resources
restrictions of large batch size. Li et al. \cite{li2016optimizing} explore the impact of data layout on convolution operations. Zhang et
al. \cite{zhang2018high} design a method to map computation to FMA (fused multiply-add) units and focus on maximizing arithmetic intensity.
Unlike our approach, none of these methods explicitly considers SM utilization, which is vital for achieving good performance for DSC on GPUs. \RV{The work presented in \cite{chen2019versatile} also targets column and row reuse. However, there are two main drawbacks in their approach. First, 32 threads of a warp in their approach will generate 28 output elements for a $5 \times 5$ filter. Our column reuse method ensures that 32 threads of a warp generate 32 output elements for any filter sizes, thus our approach needs fewer warps and is more efficient than their approach. Second, their approach will load all needed input elements into registers to avoid reloading shared input elements, while our approach loads one input element each time and calculate all output elements that depend on the loaded input element. Thus, our approach uses less registers and can execute more warps on one SM concurrently.}

