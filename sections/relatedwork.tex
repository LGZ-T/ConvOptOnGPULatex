\section{Related Work}
Numerous efforts have been dedicated to optimizing convolution operations. As previously mentioned, GEMM-, FFT- and
Winograd-based convolutions are broadly adopted convolution algorithms.

GEMM-based convolution is the first attempt to optimize convolution. Chellapilla et al. \cite{Chellapilla2006High} developed an unrolling
convolution algorithm  called the im2col convolution algorithm. They lowered the convolutions into a matrix multiplication, which is
highly optimized on the GPU. Despite some duplicate input elements involved in the computation, the performance gain of this algorithm is impressive. Chetlur et al. \cite{ChetlurWVCTCS14}, who worked for NVIDIA, integrated the unrolling convolution algorithm into cuDNN.

After obtaining the satisfactory optimization of GEMM, researchers shifted their focus from GEMM-based convolution to FFT- and
Winograd-based ones. Both convolutions can reduce computational complexity and improve convolution performance. Mathieu et al.
\cite{mathieu2013fast} proposed an FFT-based convolution to compute convolutions as pointwise products in the Fourier domain and reuse the
transformed input data, which significantly reduce the complexity of the convolution, many times. This method greatly improved the
performance of convolution compared with GEMM-based implementation. However, FFT-based convolution is more suitable for large filters than
for small ones because padding the filters to the same size as the input data is necessary, and  the latter (e.g., 3 Ã— 3 filters) needs
more memory than the former. To overcome this drawback, Lavin et al. \cite{lavin2016fast} used Winograd's minimal filtering algorithm to
accelerate the convolution on GPU. This algorithm can reduce the arithmetic complexity of a convolution by up to four times compared with
direct convolution. However, Winograd's algorithm is only suitable for small filters due to its numerical instability.
Zhen et al. \cite{Zhen2018Optimizing} extended Winograd's algorithm to support any filter size. However, the traditional and extended Winograd-based
algorithms need to transform the input and filter before performing matrix multiplication, and both require more operations compared with
the FFT algorithm.

The transformation of the input and filter before performing matrix multiplication incurs a large memory
overhead and offset some performance gains caused by the reduction in computational complexity. Therefore, recent studies have mainly focused on reduction of the the memory overhead of the transformation phases. Cho et al. \cite{cho2017mec} reduced the memory overhead of GEMM-based convolutions using a
compact lowering scheme to reduce the redundancy in the lowered matrix and then performed multiple small matrix multiplications in parallel.
However, this algorithm still needs to transform the input and filter tensors into lowered matrixes to compute the convolution.
Iandola et al. \cite{Iandola2014Communication} reduced memory communication of 2D convolutions on GPU. They also prefetched the image regions to the registers. {\color{red}Their method uses a fewer number of threads, but each thread operates on a larger number of data items.} This method uses the registers to accelerate the convolution and does not reduce the number of
memory accesses. Different from \cite{Iandola2014Communication}, we further optimize the use of registers and significantly reduce the
number of memory accesses.
