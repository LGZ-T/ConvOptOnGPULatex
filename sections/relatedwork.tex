\section{Related Work}
Numerous efforts have been dedicated to optimizing convolution operations. As previously mentioned, GEMM-, FFT- and
Winograd-based convolutions are broadly adopted convolution algorithms.

GEMM-based convolution is the first attempt to optimize convolution. Chellapilla et al. \cite{Chellapilla2006High} developed an unrolling
convolution algorithm  called the im2col convolution algorithm. They lowered the convolutions into a matrix multiplication, which is
highly optimized on the GPU. Despite some duplicate input elements involved in the computation, the performance gain of this algorithm is impressive. Chetlur et al. \cite{ChetlurWVCTCS14}, who worked for NVIDIA, integrated the unrolling convolution algorithm into cuDNN.

After obtaining the satisfactory optimization of GEMM, researchers shifted their focus from GEMM-based convolution to FFT- and
Winograd-based ones. Both convolutions can reduce computational complexity and improve convolution performance. Mathieu et al.
\cite{mathieu2013fast} proposed an FFT-based convolution to compute convolutions as pointwise products in the Fourier domain and reuse the
transformed input data, which significantly reduce the complexity of the convolution, many times. This method greatly improved the
performance of convolution compared with GEMM-based implementation. However, FFT-based convolution is more suitable for large filters than
for small ones because padding the filters to the same size as the input data is necessary, and  the latter (e.g., 3 Ã— 3 filters) needs
more memory than the former. To overcome this drawback, Lavin et al. \cite{lavin2016fast} used Winograd's minimal filtering algorithm to
accelerate the convolution on GPU. This algorithm can reduce the arithmetic complexity of a convolution by up to four times compared with
direct convolution. However, Winograd's algorithm is only suitable for small filters due to its numerical instability.
Zhen et al. \cite{Zhen2018Optimizing} extended Winograd's algorithm to support any filter size. However, the traditional and extended Winograd-based
algorithms need to transform the input and filter before performing matrix multiplication, and both require more operations compared with
the FFT algorithm.

The transformation of the input and filter before performing matrix multiplication incurs a large memory overhead and offset some
performance gains caused by the reduction in computational complexity. Therefore, recent studies have mainly focused on reduction of the
the memory overhead of the transformation phases. Cho et al. \cite{cho2017mec} reduced the memory overhead of GEMM-based convolutions using
a compact lowering scheme to reduce the redundancy in the lowered matrix and then performed multiple small matrix multiplications in
parallel. However, this algorithm still needs to transform the input and filter tensors into lowered matrixes to compute the convolution.
Iandola et al. \cite{Iandola2014Communication} reduced memory communication of 2D convolutions on GPU. They also prefetched the image
regions to the registers. While their method uses fewer threads, each thread operates on a larger number of data items. As a result, their
method does not reduce the number of global memory transactions. Unlike \cite{Iandola2014Communication}, we further optimize the use of
registers and significantly reduce the number of memory accesses.
