\section{Evalution}
\label{exp}
We evaluate our implementations of convolution on two platforms: (a) NVIDIA Tesla K40m facilitates 2880 FP32 cores with 64KB L1 cache and
shared memory. The host machine has a 2.40GHz Intel Xeon E5-2620 CPU with 128GB memory and Linux kernel v4.19.85. (b) NVIDIA RTX 2080 Ti
facilitates 4350 FP32 cores and 4350 INT32 cores with 96KB L1 cache and shared memory. The host machine has a 2.30GHz Intel Xeon E5-2697
CPU with 252GB memory and Linux kernel v4.15.0. Both platforms are shipped with the CUDA Toolkit v10.1. We use following state-of-the-art
convolution libraries as a comparison on both platforms:
\begin{itemize}
  \item cuDNN, version 7.6. cuDNN is the state-of-art convolution implementation. It supports both 2D and 3D convolutions on GPU.
      Moreover, cuDNN implements GEMM-based, FFT-based and Winograd-based convolutions.
  \item GEMM-im2col and GEMM-im2row (im2col and im2row for short). Both are GEMM-based convolutions and support 2D and 3D convolutions.
      We extract the implementations of im2col and im2row from Caffe \cite{jia2014caffe} and MatConvNet \cite{vedaldi15matconvnet}
      respectively.
  \item ArrayFire \cite{Yalamanchili2015}, version 3.6.4. ArrayFire is a popular image and signal processing library. It implements 2D
      convolution on GPU and can be easily invoked through an api. The semantics of 3D convolution of ArrayFire is not compatible with 3D
      convolutions in CNNs. Therefore we use ArrayFire only for 2D convolution. ArrayFire uses Just In Time (JIT) compiling for standard
      arithmetic operations, which means that the first run of an ArrayFire application will take a much longer time than the second run.
      Thus, we run ArrayFire twice in each test and take the second runtime.
  \item NVIDIA Performance Primitives (NPP), version 10.1. NPP is an image and signal processing library. We do not find functions for
      performing 3D convolution like cuDNN. Therefore we use NPP only for 2D convolution.

\end{itemize}

After years of updating, cuDNN has integrated seven widely used convolution algorithms. Therefore, cuDNN provides an option that uses
heuristics to find the most suitable algorithm for a convolution. We set the option to $PREFER\_FASTEST$ (the exact option can be found at
\cite{CUDAtoolkit}) meaning that we want to use the fastest algorithm without regarding the memory capacity. However, the algorithm
selected by the heuristic is not always the fastest. Therefore, we use two types of runtime for cuDNN. One is the real fastest runtime
among seven algorithms and the other is heuristically fastest. For 2D convolution, we use the real fastest runtime since two types of
runtime are the same.

%Our implementations of convolution use only a small part of shared memory and mainly relay on GPU L1 cache. Therefore we employ $cudaDeviceSetCacheConfig$ function to set GPU cache policy with arguments $cudaFuncCachePreferL1$ for our convolutions.
All experiments use 32-bit float data type and use the average runtime of 10 iterations. All data are 4-dimension tensors $(N,C,H,W)$. For
now, we implement a 2D convolution and 3D convolutions with one and three input channels for filters of size $3 \times 3$ and $5 \times 5$,
since small filters are commonly used in applications. We first present results for 2D convolution and then 3D convolution.

\subsection{2D Convolution}
\begin{figure*}
\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/2d_norm_f3.eps}
		 \caption{Speedups for the filter of size $3 \times 3$ on Tesla K40m.}
		 \label{fig:2druntimef3c1}
	\end{subfigure}
	\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/2d_norm_f5.eps}
		 \caption{Speedups for the filter of size $5 \times 5$ on Tesla K40m.}
		 \label{fig:2druntimef5c1}
	\end{subfigure}
	
	\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/2d_norm_f3_rtx2080.eps}
		 \caption{Speedups for the filter of size $3 \times 3$ on RTX 2080 Ti.}
		 \label{fig:2druntimef3c12080}
	\end{subfigure}
	\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/2d_norm_f5_rtx2080.eps}
		 \caption{Speedups for the filter of size $5 \times 5$ on RTX 2080 Ti.}
		 \label{fig:2druntimef5c12080}
	\end{subfigure}
	
	\caption{Speedups of our implementation over the other five 2D convolution implementations on two platforms.}
   \label{fig:2druntime}
\end{figure*}

\begin{table*}[]
\caption{Summation of memory transactions on each type of memory, including global memory, texture memory and shared memory. Data is collected on Tesla K40m.}
\label{tab:2dmemtrans}
\begin{tabular}{c|ccccc|ccccc}
\hline
\multicolumn{1}{l|}{}                                  & \multicolumn{5}{c|}{Filter size $3 \times 3$}                                                & \multicolumn{5}{c}{Filter size $5 \times 5$}                                                \\ \hline
                                                       & $256 * 256$      & $512 * 512$      & $1K * 1K$        & $2K * 2K$        & $4K * 4K$        & $256 * 256$      & $512 * 512$      & $1K * 1K$        & $2K * 2K$        & $4K * 4K$        \\ \hline
cuDNN                                                  & 6.0E+05          & 2.4E+06          & 9.7E+06          & 3.9E+07          & 1.5E+08          & 1.4E+06          & 5.6E+06          & 2.2E+07          & 8.9E+07          & 3.6E+08          \\
 im2col & 1.1E+05          & 4.4E+05          & 4.4E+05          & 7.0E+06          & 2.8E+07          & 2.3E+05          & 9.3E+05          & 3.7E+06          & 1.5E+07          & 6.0E+07          \\
im2row & 1.1E+05          & 4.4E+05          & 4.4E+05          & 7.0E+06          & 2.8E+07          & 2.3E+05          & 9.3E+05          & 3.7E+06          & 1.5E+07          & 6.0E+07          \\
ArrayFire                                              & 4.9E+04          & 2.0E+05          & 7.9E+05          & 3.2E+06          & 1.3E+07          & 1.1E+05          & 4.2E+05          & 1.7E+06          & 6.8E+06          & 2.7E+07          \\
NPP                                                    & 6.0E+04          & 2.4E+05          & 9.6E+05          & 3.8E+06          & 1.5E+07          & 1.3E+05          & 5.2E+05          & 2.1E+06          & 8.3E+06          & 3.3E+07          \\
Ours                                                   & \textbf{7.5E+03} & \textbf{2.7E+04} & \textbf{1.1E+05} & \textbf{4.2E+05} & \textbf{1.6E+06} & \textbf{1.0E+04} & \textbf{3.3E+04} & \textbf{1.3E+05} & \textbf{4.9E+05} & \textbf{1.9E+06} \\ \hline
\end{tabular}
\end{table*}

In this section, we report the performance of 2D convolution obtained from six implementations, including cuDNN, im2col, im2row, ArrayFire,
NPP and our implementation. We evaluate these six implementations with image size ($I_H \times I_W$) ranging from $256 \times 256$ to $4K
\times 4K$, $I_N=F_N=1$ and $I_C=F_C=1$. We first present speedups of our implementation over the other five implementations in Figure
\ref{fig:2druntime}. Then we show the effectiveness of our implementation on reducing the number of memory transactions in Table
\ref{tab:2dmemtrans}.

It is apparent from Figure \ref{fig:2druntime} that cuDNN, im2col and im2row are not suitable for 2D convolution while ArrayFire, NPP and
our implementation are well optimized for 2D convolution. Our implementation exhibits superior performance over cuDNN, im2col and im2row on
two platforms with an average of 5.9$\times$, 5.9$\times$ and 5.8$\times$ speedups respectively.

Figure \ref{fig:2druntimef3c1} and \ref{fig:2druntimef5c1} show speedups on Tesla K40m,  we can see that our implementation achieves the
best results in nine cases out of ten. Our implementation is only slower than NPP when performing $3 \times 3$ convolution on a $256 \times
256$ image. The average speedups of our implementation over ArrayFire and NPP on Tesla K40m are 2.1$\times$ and 2.9$\times$ respectively.
Figure \ref{fig:2druntimef3c12080} and \ref{fig:2druntimef5c12080} show speedups on RTX 2080 Ti, our implementation achieves the best
results in six cases out of ten. We can see that NPP achieves the best results on small image sizes while our implementation achieves the
best results on large image sizes. The average speedups of our implementation over ArrayFire and NPP on RTX 2080 Ti are 3.1$\times$ and
1.3$\times$ respectively.

Our implementation of 2D convolution is derived from direct convolution. We employ only two optimization algorithms, column reuse
(Algorithm \ref{algo:basic}, Algorithm \ref{algo:basic2}) and row reuse (Algorithm \ref{algo:rowreuse}) on direct convolution, therefore
performance gains are mainly attributed to the reduction on the number of memory transactions. We use \emph{nvprof} provided by CUDA
Toolkit to collect the number of memory transactions for different type of memories. GPU global memory is used to store input data in
im2col, im2row, ArrayFire and our implementation. NPP and cuDNN use texture memory to store input data. Shared memory is normally used to
store filter data. Therefore we sum up the number of read transactions of these three memory types and report the results in Table
\ref{tab:2dmemtrans}. To save space, we only show the result of memory transactions collected on Tesla K40m, the result on RTX 2080 Ti
exhibits the similar trend.

From Table \ref{tab:2dmemtrans} we can see that our optimization algorithms significantly reduce the number of memory transactions.
Compared with ArrayFire and NPP which are well optimized for 2D convolution, our implementation can averagely reduce memory transactions by
a factor of 10.2 and 12.3 respectively.


% Please add the following required packages to your document preamble:
% \usepackage{multirow}


%\begin{table*}[]
%\caption{2d speedup}
%\label{tab:2dspeedup}
%\begin{tabular}{c|ccccc|ccccc}
%\hline
%\multicolumn{1}{c}{}&\multicolumn{5}{c}{$F_H*F_W= 3*3$} &\multicolumn{5}{c}{$F_H*F_W= 5*5$}\\
%\hline
%           & 256*256&512*512&1K*1K&2K*2K&4K*4K&256*256&512*512&1K*1K&2K*2K&4K*4K\\
%\hline
%cuDNN      & 1.91 & 3.74 & 5.69 & 8.07 & 7.72     & 2.86 & 4.86 & 6.08 & 7.40 & 9.20  \\
%im2col     & 1.52 & 3.25 & 5.25 & 7.56 & 7.24     & 2.34 & 4.20 & 5.37 & 6.88 & 8.38  \\
%im2row     & 1.55 & 3.25 & 5.20 & 7.50 & 7.19     & 2.34 & 4.15 & 5.32 & 6.83 & 8.32  \\
%ArrayFire  & 1.23 & 3.67 & 2.59 & 2.37 & 1.95     & 1.40 & 3.12 & 1.73 & 1.43 & 1.57  \\
%NPP        & 0.68 & 1.40 & 2.21 & 3.15 & 3.03     & 1.52 & 2.91 & 3.79 & 4.66 & 5.82 \\
%\hline
%\end{tabular}
%\end{table*}
In summary, our optimization algorithms can significantly reduce the number of memory transactions and improve the performance of 2D
convolution. Compared with state-of-the-art image processing libraries, ArrayFire and NPP, our implementation achieves average speedups of
2.1$\times$ and 2.9$\times$ on Tesla K40m, 1.3$\times$ and 3.1$\times$ on RTX 2080 Ti.

\subsection{3D Convolution}
\label{3dconvexp}
\begin{table}[]
\caption{Configurations of 3D convolution}
\label{tab:3dconvconfigs}
\begin{tabular}{c|ccccc}
\hline
& $I_N$ & $I_C=F_C$ & $I_H*I_W$ & $F_N$ & $F_H*F_W$ \\
\hline
CONV1 & 128  & 1,3       & 28*28     & 128  & 3*3       \\
CONV2 & 128  & 1,3       & 56*56     & 64   & 3*3       \\
CONV3 & 128  & 1,3       & 12*12     & 64   & 5*5       \\
CONV4 & 128  & 1,3       & 14*14     & 16   & 5*5       \\
CONV5 & 128  & 1,3       & 24*24    & 256  & 5*5       \\
CONV6 & 128  & 1,3       & 24*24     & 64   & 5*5       \\
CONV7 & 128  & 1,3       & 28*28     & 16   & 5*5       \\
CONV8 & 128  & 1,3       & 28*28     & 512   & 3*3       \\
CONV9 & 128  & 1,3       & 56*56     & 256  & 3*3       \\
CONV10 & 128  & 1,3       & 112*112     & 128   & 3*3       \\
CONV11 & 128  & 1,3       & 224*224     & 64   & 3*3      \\
\hline
\end{tabular}
\end{table}

\begin{figure*}
	
		%\centering
		 %\includegraphics[width=18cm,height=5cm]{./figure/3d_norm_c1.eps}
		 %\caption{Normalized runtime of five implementations for 3D convolution. Left and right parts of the figure is for 3D convolutions with one and three input channels.}
		 %\label{fig:3druntime}
		
	\begin{subfigure}{18cm}
		\centering
		 \includegraphics[width=18cm,height=5.7cm]{./figure/3d_norm_c1.eps}
		 \caption{Speedups on Tesla K40m, left is for one channel and right is for three channels.}
		 \label{fig:3druntimeK40}
	\end{subfigure}
	
	\begin{subfigure}{18cm}
		\centering
		 \includegraphics[width=18cm,height=5.7cm]{./figure/3d_norm_c1_rtx2080.eps}
		 \caption{Speedups on RTX 2080 Ti, left is for one channel and right is for three channels.}
		 \label{fig:3druntime2080}
	\end{subfigure}
	
	\caption{Speedups of our implementation over the other four implementations for 3D convolution with one and three input channels.}
	\label{fig:3druntime}
\end{figure*}
%\begin{figure*}
%	\begin{subfigure}{\columnwidth}
%		\centering
%		 \includegraphics[width=\columnwidth,height=5cm]{./figure/3d_norm_c3.eps}
%		 \caption{Normalized runtime for convolutions with one input channel.}
%		 \label{fig:3druntimec1}
%	\end{subfigure}
%	\begin{subfigure}{\columnwidth}
%		\centering
%		 \includegraphics[width=\columnwidth,height=5cm]{./figure/3d_norm_c1.eps}
%		 \caption{Normalized runtime for convolutions with three input channels.}
%		 \label{fig:3druntimec3}
%	\end{subfigure}
%	
%	\caption{Runtime comparison among five implementations of 3D convolutions. Each runtime is normalized to our implementation.}
%   \label{fig:3druntime}
%\end{figure*}

\begin{table*}[]
\caption{The number of memory transactions for 3D convolution. Data is collected on Tesla K40m.}
\label{tab:3dtrans}
\begin{tabular}{c|ccccc|ccccc}
\hline
\multicolumn{1}{l|}{} & \multicolumn{5}{c|}{$I_C=F_C=1$}                                                                                                                                                                                                                                                                  & \multicolumn{5}{c}{$I_C=F_C=3$}                                                                                                                                                                                                                                                                  \\ \hline
    & \begin{tabular}[c]{@{}c@{}}cuDNN\\ fastest\end{tabular} & \begin{tabular}[c]{@{}c@{}}cuDNN\\ heuristic\end{tabular} & im2col & im2row & \begin{tabular}[c]{@{}c@{}}our 3D\\ conv\end{tabular} & \begin{tabular}[c]{@{}c@{}}cuDNN\\ fastest\end{tabular} & \begin{tabular}[c]{@{}c@{}}cuDNN\\ heuristic\end{tabular} & im2col & im2row & \begin{tabular}[c]{@{}c@{}}our 3D\\ conv\end{tabular} \\ \hline
CONV1& 1.7E+06& 1.2E+07& 2.3E+06& 2.3E+06& \textbf{8.5E+05}& 3.9E+06& 1.3E+07& 5.5E+06& 5.5E+06& \textbf{2.4E+06}\\
CONV2& 6.9E+06& 6.9E+06& 4.8E+06& 4.8E+06& \textbf{9.7E+05}& 1.6E+07& 1.6E+07& 1.2E+07& 1.2E+07& \textbf{2.9E+06}\\
CONV3& 3.5E+05& 1.9E+05& 1.9E+05& 2.5E+05& \textbf{8.4E+04}& 9.5E+05& 5.2E+05& 5.2E+05& 6.8E+05& \textbf{2.5E+05}\\
CONV4& 2.7E+05& 2.4E+05& 3.0E+05& 3.6E+05& \textbf{2.4E+04}& 7.4E+05& 6.4E+05& 8.0E+05& 9.8E+05& \textbf{7.3E+04}\\
CONV5& 8.7E+06& 4.5E+06& 5.1E+06& 6.5E+06& \textbf{1.4E+06}& 1.2E+07& 1.2E+07& 1.4E+07& 1.8E+07& \textbf{4.1E+06}\\
CONV6& 2.2E+06& 1.2E+06& 1.3E+06& 1.7E+06& \textbf{3.4E+05}& 5.9E+06& 3.2E+06& 3.6E+06& 4.6E+06& \textbf{1.0E+06}\\
CONV7& 1.6E+06& 1.6E+06& 1.3E+06& 1.6E+06& \textbf{1.3E+05}& 1.7E+06& 4.3E+06& 3.6E+06& 4.5E+06& \textbf{3.9E+05}\\
CONV8& 1.3E+07& 2.4E+07& 4.7E+06& 4.8E+06& \textbf{3.4E+06}& 3.0E+07& 2.9E+07& 1.1E+07& 1.2E+07& \textbf{9.8E+06}\\
CONV9& 2.7E+07& 5.5E+07& 9.9E+06& 1.0E+07& \textbf{3.9E+06}& 6.5E+07& 5.7E+07& 2.4E+07& 2.4E+07& \textbf{1.2E+07}\\
CONV10& 3.1E+07& 1.1E+08& 2.0E+07& 2.0E+07& \textbf{1.6E+07}& 7.1E+07& 1.2E+08& 5.0E+07& 5.1E+07& \textbf{4.3E+07}\\
CONV11& 1.2E+08& 1.2E+08& 7.5E+07& 7.5E+07& \textbf{2.6E+07}& 2.8E+08& 2.7E+08& 2.0E+08& 2.0E+08& \textbf{7.1E+07}\\ \hline
\end{tabular}
\end{table*}

We also apply Algorithm \ref{algo:basic}, Algorithm \ref{algo:basic2} and Algorithm \ref{algo:rowreuse} on 3D convolution. Nowadays, the
most commonly used scenario of 3D convolution is CNNs. Therefore we implement a 3D convolution with one and three input channels. The main
focus of this work is to optimize memory transactions not the implementation of convolution. Thus, our convolutions do not optimize on
input channels. As shown in Algorithm \ref{algo:overalldesign}, our implementation is a linear scale with the number of input channels.
Therefore, our implementation is more suitable for convolutions with one and three input channels, which are normally the first layers of a
CNN.

We present performance comparison of 3D convolution among four implementations, including cuDNN, im2col, im2row and our implementation. We
evaluate the performance of 3D convolution from three aspects, the speedup, the number of memory transactions and the memory consumption.
For cuDNN, we obtain two types of runtime. First, we run all algorithms provided in cuDNN and obtain the fastest runtime, which is denoted
as \emph{cuDNN fastest}. Second, we run cuDNN without specifying the algorithm, in which case cuDNN uses a heuristic method to find the
most suitable algorithm for a convolution configuration. The runtime generated by the heuristic method is denoted as \emph{cuDNN
heuristic}. The reason to obtain the heuristic runtime is that popular machine learning frameworks, such as Pytorch, TensorFlow and Caffe,
use cuDNN with  the heuristic method in their implementations. Pythorch also uses the fastest algorithm of cuDNN in some cases.

\begin{figure*}
	
	\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/mem3d_1.eps}
		 %\caption{Global memory usage for five implementations on Tesla K40m.}
		 \label{fig:3dmemk40m}
	\end{subfigure}
	\begin{subfigure}{\columnwidth}
		\centering
		 \includegraphics[width=\columnwidth,height=6cm]{./figure/mem3d_1_rtx2080.eps}
		 %\caption{Global memory usage for five implementations on RTX 2080 Ti.}
		 \label{fig:3dmemrtx2080}
	\end{subfigure}
	\caption{Global memory usage for five implementations. Left figure demonstrates the result on Tesla K40m and right figure demonstrates the result on RTX 2080 Ti.}
	\label{fig:3dmem}
\end{figure*}

We collect configurations of convolutional layers with filters of size $3 \times 3$ and $5 \times 5$ from four popular CNN models,
including AlexNet \cite{Krizhevsky2012ImageNet}, VGG \cite{SimonyanZ14a}, ResNet \cite{HeZRS16} and GoogleLeNet \cite{SzegedyLJSRAEVR15}.
Then we set the number of input channels to one and three ($I_C=F_C \in \{1, 3\}$) and the batch size to 128 ($I_N=O_N=128$). Other batch
sizes have the similar performance because all tested implementations have a linear scale as the batch size. Exact configuration is shown
in Table \ref{tab:3dconvconfigs}.

We test all 3D convolution implementations on two platforms, speedups of our implementation over the other four implementations are shown
in Figure \ref{fig:3druntime}. We can see that im2col and im2row perform poorly in most cases, especially on RTX 2080 Ti. Our
implementation achieves average speedups of 17.9$\times$ and 18.8$\times$ over im2col and im2row on two platforms. Figure
\ref{fig:3druntimeK40} shows speedups on Tesla K40m, though cuDNN is highly optimized for 3D convolution on GPU, our implementation can
still obtain an average speedup of 1.5$\times$ and 2.3$\times$ with respect to cuDNN fastest and cuDNN heuristic respecitively. Figure
\ref{fig:3druntime2080} shows speedups on RTX 2080 Ti, compared with cuDNN fastest, we achieve an average speedup of 1.2$\times$ and the
maximum speedup can be up to 2.5$\times$. The speedup over cuDNN heuristic is much higher, it can be 2.2$\times$ on average and the maximum
speedup can be up to 6.2$\times$.

Different from 2D convolution, the performance gains of 3D convolution are obtained from two aspects, reduction on memory transactions of
input data and filters. Once we load a filter into shared memory, we try to use it to slide over as many images as possible. In this way we
do not need to load the same filter for each input. In Table \ref{tab:3dtrans}, we show the number of memory transactions for four
implementations on Tesla K40m. We use the same method as 2D convolution to collect the number of memory transactions. Our approach achieves
the minimum transaction counts in all tested cases and can averagely reduce the number of memory transactions by a factor of 6.1 and 4.4
with respect to \emph{cuDNN fastest} for one and three input channels respectively. Compared with transaction counts in 2D convolution
(Table \ref{tab:2dmemtrans}), transaction counts in 3D convolution have a lower ratio with respect to other implementations. The reason is
that in 3D convolution, each filter needs to slide over all inputs and thus we need to load the same filter multiple times. Though we have
optimized the usage of filters, we still need to load the filters multiple times since the capacity of shared memory can not fit the whole
filters. As a consequence, the number of memory transactions increases.

For 3D convolution, memory usage is also important for the scarce memory storage in GPU. We use \emph{nvidia-smi} command provided by
NVIDIA CUDA Toolkit to record the peak GPU device memory usage when the application is running. We report global memory usage only for
convolutions with one input channel. The reason is that output data consumes most memory storage compared with input and filter data,
changing the number of input channels alone does not change the size of output data. Therefore memory usage has little change when we
change the number of input channels alone.

Figure \ref{fig:3dmem} demonstrates the memory usage of four implementations on two platforms. Our implementation is derived from direct
convolution which does not require extra memory, thus our implementation consumes the minimum memory storage. GEMM based implementations
(im2col and im2row) first transform filters into a large matrix and then transform one input into a matrix each time they perform a matrix
multiplication. Therefore, they only need extra memory to store transformed filters and one transformed input. The most memory consuming
implementation is cuDNN. Since cuDNN is closed source, we could only guess that it improve the performance at a cost of memory usage. The
memory consumption of our implementation is reduced by 335MB on average, 384MB  for maximum compared with cuDNN fastest. The commonly used
cuDNN heuristic consumes even more memory than cuDNN fastest. Compared with cuDNN heuristic, our implementation reduces the memory
consumption by 442MB on average and the maximum reduction can be up to 876MB.

In summary, our optimization algorithms successfully reduce the number of memory transactions and improve the performance of 3D convolution
on two platforms. Our implementation does not need any extra memory while cuDNN consumes a large amount of memory to accelerate convolution
operation, especially for cuDNN heuristic.
%From Table \ref{tab:3dspeedup} we can see that as the channel number increases, the speedup of cuDNN, im2col and im2row decrease obviously. We analyze the algorithms of im2col and im2row. Both transform the channels into a large matrix and the use the highly optimized gemm library to do the real work. When we increase the number of channels, the size of the transformed matrix also increases. But the gemm library can distribute the work among gpu cores very well, which brings little overhead on each gpu core. Though without the source code of cuDNN, we use $nvprof$ to analyze cuDNN and find that cuDNN also utilizes gemm library to do the real work. Therefore the reason also holds for cuDNN. In our implementation, we loop over each channel to calculate the final result. As the channel number increases, out implementation shows a linear scale as channel number. Therefore, as the channel number increases, the speed up of our implementation over cuDNN, im2col and im2row decreases. ArrayFire uses a similar method as our implementation, the speedup over ArrayFire is not decreased. Further optimization on channel number will be explored in the future. In this work, we mainly focus on the effectiveness of our two reuse algorithms not the implementation of the convolution.

%Second, compare through 3*3 and 5*5, we can find that for cudnn im2col im2row npp 's sppedup keep same across different filter size, but ArrayFire decreasees, the reason is that when compailing for filter 5, we have to reduce the number of each thread. or it will generate
