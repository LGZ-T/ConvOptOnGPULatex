

\section{Experimental Setup}

\subsection{Evaluation Platforms} We evaluate our approach on two platforms. 
One is NVIDIA RTX 2080Ti GPU (2080Ti), which integrates 4350 CUDA cores for floating point computation and 4350 CUDA cores for integer operations. 
The GPU has a 64KB shared memory. 
The host machine has a 2.30GHz Intel Xeon E5-2697 CPU with 252GB memory, running Linux kernel v4.15.0. We use CUDA Toolkit version 10.2.
The other is NVIDIA Jetson AGX Xavier GPU (Xavier), which integrates 512 Volta cores and 48KB shared memory.
The host machine has an 8-core Arm CPU with 32GB memory, running Linux kernel v4.15.0. We use CUDA Toolkit 10.0. 


\subsection{Competing Methods} We compare our approach against the following state-of-the-art convolution libraries:
\begin{itemize}
  \item \textbf{cuDNN version 7.6.4}. cuDNN is a state-of-the-art convolution implementation that supports all kinds of convolutions including depthwise and pointwise convolutions on GPU. 
  Moreover, cuDNN can execute GEMM-, FFT- and Winograd-based convolutions. We take cuDNN GEMM implementation as a baseline for pointwise convolution.
  \item \textbf{Direct implementation of depthwise convolution}. We implement a direct depthwise convolution without using the proposed reuse algorithms. We take this implementation as a baseline for depthwise convolution.
\end{itemize}

\subsection{Use Cases}
We apply our approach to depthwise convolution and pointwise convolution, described as follows.

\mypara{Depthwise convolution.} This applies a bank of single-channel 2D filters to convolve with one multi-channel 2D input, e.g., an image of three color channels, R, G and B. We apply a 2D convolution filter to each of the input channels. 
This technique has been widely used in many CNNs, including MobileNetv2 \cite{Sandler_2018_CVPR}, EfficientNet \cite{tan2019efficientnet} and ShuffleNetv2 \cite{Ma_2018_ECCV}.

\mypara{Pointwise convolution.} This applies a bank of multi-channel 2D 1$\times$1 filters to convolve with one multi-channel 2D input. 
The 2D convolution is performed between one filter channel and the corresponding input channel, and the results are sum up across channels to generate one output channel.


\subsection{Performance Report}
We run each test case ten times with batch sizes 1, 8, 16, 32, 64 and 128 on an unloaded machine and report the averaged running time. 
We found little variance during execution runs, less than 2\%.  
All the convolutions used in evaluation use the standard 32-bit (single-precision) floating point numbers, and the data are organized as 4D tensors $(N,C,H,$ and $W)$, where $N$, $C$, $H$, and $W$ are the batch size, channel, height, and width, respectively. 
In this work, we first test depthwise convolution with two filter sizes, $3 \times 3$ and $5 \times 5$, because these are commonly used filter sizes. 
Then we report the performance of pointwise convolution. 
Lastly, we apply our optimized depthwise and pointwise convolutions on MobileNetv2 and report the performance of inference and training.
%
%\subsection{Notations}
%Throughout the evaluation, we use $I$, $F$, and $O$ to represent the input, the filter, and the output respectively, $N$, $C$, $H$, and $W$
%to denote the batch size, the channel, the height, and the width, respectively.

\section{Experimental Results}
\label{exp} In this section, we report our results for depthwise convolution (Section \ref{sec:depconvexp}), pointwise convolution (Section \ref{sec:pwconvexp}) and inference and training of MobileNetV2 (Section \ref{sec:inferexp}), showing that our approach consistently outperforms alternative methods by delivering the overall best performance.


%\subsection{2D Convolution\label{sec:ex2dc}}
%\begin{figure*}
%\centering
%\subfloat[][$3 \times 3$ filter]{\includegraphics[width=0.85\columnwidth]{./figure/2d_conv_f3.eps}
%	\label{fig:2druntimef3c12080}}
%\hspace{0em}
%\subfloat[][$5 \times 5$ filter]{\includegraphics[width=0.85\columnwidth]{./figure/2d_conv_f5.eps}
%	\label{fig:2druntimef5c12080}}
%
%\vspace{-1mm}	\caption{Speedups of 2D convolutions of four implementations over GEMM-im2col when using a $3 \times 3$ (a) and a $5 \times
%5$ filter (b).} \label{fig:2druntime} \vspace{-3mm}
%\end{figure*}
%
%
%\subsubsection{Setup}
%In this experiment, we compare our approach against the 2D convolution implementations from cuDNN, GEMM-im2col, ArrayFire, and
%NPP. As cuDNN provides multiple implementations, we empirically choose the fastest version, denoted as cuDNN-fastest, for evaluation. We
%apply each method to images with sizes ranging from $256 \times 256$ to $4K \times 4K$. We set the batch size, channel, height, and width
%of the input to be 1.
%
%
%\subsubsection{Overall results}
% Figure
%\ref{fig:2druntime} reports the speedups of cuDNN, ArrayFire, NPP and our approach over GEMM-im2col. While cuDNN has been heavily optimized
%for NVIDIA GPUs, it does not show a notable performance advantage. When using a $3 \times 3$ filter, our approach gives the best overall
%speedup of 5.4$\times$ (up to 9.7$\times$ for the largest input), which translates to an improvement of  more than 30\% over the second-best method, NPP.
%We note that our approach is based on the standard 2D direct convolution by applying the column and row reuse algorithms. Therefore, the
%performance gain is mainly attributed to the reduction of the number of memory transactions. When using a $5 \times 5$ filter, our approach
%achieves a better overall speedup of $7.7\times$. A $5 \times 5$ filter has four overlapped columns and rows (instead of just one
%overlapped column and row on width and height dimensions when using a $3 \times 3$ filter). The larger number of overlapped data thus gives
%more room for performance optimization when using our column and row reuse algorithms. Our approach achieves the best overall performance
%and demonstrates significant performance advantages when processing large input.
%
%\subsubsection{Further analysis}
%\begin{figure}[t!]
%\centering
%
%\subfloat[][Memory throughput]{\includegraphics[width=0.85\columnwidth]{./figure/2dmemthroughput.eps}
%	\label{fig:2dmemthr}}
%
%\subfloat[][Maximum bandwidth]{\includegraphics[width=0.85\columnwidth]{./figure/2dmembandwidth.eps}
%	\label{fig:2dmaxband}}
%	
%\caption{Memory throughput (a) and maximum bandwidth (b) of NPP and our implementation for the filter of size $3 \times 3$.}
%\label{fig:2dmemanaly}
%\vspace{-5mm}
%\end{figure}
%
%On the test cases of small input sizes (i.e., $256 \times 256$ and $512 \times 512$), NPP gives the best performance. To understand the
%performance gap on small input sizes, we use \emph{nvidia compute} to collect memory profiles, including memory throughput and max
%bandwidth, of our implementation and NPP when using a $3 \times 3$ filter to perform convolution. The profiling results are shown in Figure
%\ref{fig:2dmemanaly}.  We see that as the input size increases, the memory throughput and the maximum bandwidth also grow. This is expected
%as the larger the input image, the larger number the memory transactions is likely to be when performing convolution.  We also observe that
%our approach leads to higher memory throughput and the maximum bandwidth used when the input image size is greater or equal than $1K \times
%1K$. This finding matches the results presented in Figure \ref{fig:2druntime}, where our approach gives the best performance when
%processing an image whose size is $1K \times 1K$ or larger. This result also suggests that memory performance has a strong correlation with
%the performance of 2D convolutions.
%
%
%The reason why our approach gives lower memory throughput and achieves lower maximum bandwidth on small input sizes over NPP is explained
%as follows. For 2D convolution, only one filter is used to convolve with one single-channel input feature map, which
%requires much less computation compared to, e.g., the depth-wise or multi-channel 2D convolutions. Therefore, memory performance has a huge
%impact on the performance of 2D convolutions. Our row reuse algorithm performs better when a thread operates on more rows of output.
%However, the more rows a thread compute on, the fewer warps and thread blocks we can generate. Without enough warps issuing memory
%requests, the memory throughput and maximum bandwidth will reduce significantly, which leads to slower performance over NPP. As the input
%size increases, we can allocate more thread blocks and more warps per thread block. With enough memory requests and a reduction in redundant
%memory transactions, we can increase the memory throughput and max bandwidth to improve the performance of 2D convolutions.
%
%\subsubsection{Summary} Overall, our optimization algorithms can greatly reduce the number of memory transactions and improve the performance of 2D
%convolutions. Compared with state-of-the-art image processing libraries, NPP, our approach achieves average speedups of 1.4$\times$ and
%1.3$\times$ when using a $3 \times 3$ and $5 \times 5$ filters, respectively.

\subsection{Depthwise Convolution}
\label{sec:depconvexp}

\begin{table}[]
\caption{Layer configurations of depthwise  convolutions.}
\label{tab:depconvconfigs}
\centering
\rowcolors{2}{}{Gray}
\begin{threeparttable}
\begin{tabular}{lrrrrr}
\toprule
& \textbf{$I_N$} & \textbf{$I_C$} & \textbf{$I_H \times I_W$ }&  \textbf{$F_H \times F_W$} &\textbf{$S$}\\
\midrule
\textbf{CV1} & 1,8,16,32,64,128  & 16    & 112$\times$112 & $3 \times 3$, $5 \times 5$&2  \\
\textbf{CV2} & 1,8,16,32,64,128  & 72    & 56$\times$56  &$3 \times 3$, $5 \times 5$  &2 \\
\textbf{CV3} & 1,8,16,32,64,128  & 88   & 28$\times$28  &$3 \times 3$, $5 \times 5$   &1 \\
\textbf{CV4} & 1,8,16,32,64,128  & 96    & 28$\times$28  &$3 \times 3$, $5 \times 5$  &2  \\
\textbf{CV5} & 1,8,16,32,64,128  & 96   & 14$\times$14  &$3 \times 3$, $5 \times 5$   &1 \\
\textbf{CV6} & 1,8,16,32,64,128  & 120   & 14$\times$14  &$3 \times 3$, $5 \times 5$  &1  \\
\textbf{CV7} & 1,8,16,32,64,128  & 192   & 14$\times$14  &$3 \times 3$, $5 \times 5$  &1  \\
\textbf{CV8} & 1,8,16,32,64,128  & 240   & 14$\times$14  &$3 \times 3$, $5 \times 5$  &2  \\
\textbf{CV9} & 1,8,16,32,64,128  & 432   & 7$\times$7  &$3 \times 3$, $5 \times 5$    &1\\

\bottomrule
\end{tabular}
\end{threeparttable}
\vspace{-5mm}
\end{table}


\begin{table}[]
\setlength{\tabcolsep}{3.8pt}
\caption{Speedups of depthwise convolutions with a $3 \times 3$ filter on two platforms. We use ${\dag}$ and ${*}$ to denote speedups of cuDNN and ours over the baseline implementation respectively.}
\label{tab:dwconvf3}
%\rowcolors{2}{}{Gray}
\begin{threeparttable}
\begin{tabular}{c|cccccc|cccccc}
\toprule
&\multicolumn{6}{c|}{Speedups on 2080Ti} & \multicolumn{6}{c}{Speedups on Xavier}\\

\midrule
Batch& \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128} & \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128}\\
\hline
CV1$^{\dag}$&0.9 &2.1 &2.8 &3.0 &3.3 &3.5 &2.9 &2.5 &2.5 &2.5 &2.4 &2.5 \\
CV1$^{*}$&\textbf{3.3} &\textbf{4.4} &\textbf{4.2} &\textbf{4.1} &\textbf{4.1} &\textbf{4.0} &\textbf{5.6} &\textbf{3.1} &\textbf{3.5} &\textbf{3.4} &\textbf{3.4} &\textbf{3.5} \\
\hline
CV2$^{\dag}$&1.6 &2.7 &3.0 &3.3 &3.6 &3.8 &3.1 &2.6 &2.6 &3.2 &3.0 &3.2 \\
CV2$^{*}$&\textbf{3.1} &\textbf{4.5} &\textbf{4.0} &\textbf{4.2} &\textbf{3.8} &\textbf{3.9} &\textbf{4.5} &\textbf{4.0} &\textbf{4.2} &\textbf{4.8} &\textbf{4.6} &\textbf{5.1} \\
\hline
CV3$^{\dag}$&1.2 &2.2 &2.7 &3.0 &3.3 &3.5 &2.9 &2.5 &2.8 &2.9 &2.7 &2.9 \\
CV3$^{*}$&\textbf{2.2} &\textbf{4.0} &\textbf{4.6} &\textbf{4.8} &\textbf{4.7} &\textbf{4.5} &\textbf{4.2} &\textbf{3.0} &\textbf{4.0} &\textbf{4.2} &\textbf{4.9} &\textbf{5.1} \\
\hline
CV4$^{\dag}$&1.6 &2.7 &3.2 &3.7 &4.0 &4.2 &3.2 &1.8 &2.4 &2.4 &2.8 &2.8 \\
CV4$^{*}$&\textbf{2.7} &\textbf{4.7} &\textbf{5.6} &\textbf{5.7} &\textbf{5.2} &\textbf{5.2} &\textbf{3.4} &1.7 &\textbf{4.1} &\textbf{4.5} &\textbf{5.1} &\textbf{5.3} \\
\hline
CV5$^{\dag}$&1.1 &2.0 &2.6 &3.8 &4.7 &5.6 &2.7 &2.2 &4.6 &3.8 &3.5 &4.8 \\
CV5$^{*}$&\textbf{2.5} &\textbf{5.0} &\textbf{6.8} &\textbf{8.6} &\textbf{10.7} &\textbf{12.4} &\textbf{4.0} &\textbf{4.5} &\textbf{7.0} &\textbf{4.2} &\textbf{6.4} &\textbf{7.9} \\
\hline
CV6$^{\dag}$&0.8 &1.9 &2.5 &3.4 &3.9 &4.6 &2.9 &2.9 &3.6 &4.2 &4.8 &3.8 \\
CV6$^{*}$&\textbf{2.0} &\textbf{4.5} &\textbf{6.2} &\textbf{7.9} &\textbf{9.6} &\textbf{11.2} &\textbf{4.7} &\textbf{4.8} &2.7 &\textbf{4.3} &\textbf{6.7} &\textbf{6.7} \\
\hline
CV7$^{\dag}$&1.1 &1.9 &2.5 &2.9 &3.4 &3.9 &3.6 &3.1 &2.9 &2.8 &3.3 &2.9 \\
CV7$^{*}$&\textbf{2.4} &\textbf{4.9} &\textbf{5.6} &\textbf{6.7} &\textbf{7.5} &\textbf{8.0} &2.4 &\textbf{3.7} &\textbf{3.4} &\textbf{4.6} &\textbf{3.6} &\textbf{4.7} \\
\hline
CV8$^{\dag}$&1.1 &1.9 &2.3 &2.6 &2.9 &3.2 &3.1 &2.1 &3.0 &2.9 &2.6 &2.5 \\
CV8$^{*}$&\textbf{2.6} &\textbf{4.5} &\textbf{5.0} &\textbf{5.8} &\textbf{6.4} &\textbf{6.5} &\textbf{3.4} &\textbf{4.1} &\textbf{3.4} &\textbf{4.3} &\textbf{2.9} &\textbf{3.9} \\
\hline
CV9$^{\dag}$&1.0 &1.9 &2.4 &3.3 &4.2 &4.8 &3.7 &1.7 &2.4 &4.6 &4.7 &5.2 \\
CV9$^{*}$&\textbf{2.5} &\textbf{4.4} &\textbf{5.7} &\textbf{7.4} &\textbf{8.7} &\textbf{10.0} &\textbf{4.3} &\textbf{1.8} &\textbf{3.3} &3.9 &\textbf{5.6} &4.8 \\

\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

\begin{table}[]
\setlength{\tabcolsep}{3.8pt}
\caption{Speedups of depthwise convolutions with a $5 \times 5$ filter on two platforms. We use ${\dag}$ and ${*}$ to denote speedups of cuDNN and ours over the baseline implementation respectively.}
\label{tab:dwconvf5}
%\rowcolors{2}{}{Gray}
\begin{threeparttable}
\begin{tabular}{c|cccccc|cccccc}
\toprule
&\multicolumn{6}{c|}{Speedups on 2080Ti} & \multicolumn{6}{c}{Speedups on Xavier}\\

\midrule
Batch& \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128} & \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128}\\
\hline
CV1$^{\dag}$&1.1 &1.4 &1.4 &1.5 &1.5 &1.5 &1.5 &1.2 &1.2 &1.1 &1.0 &1.0 \\
CV1$^{*}$&\textbf{3.5} &\textbf{6.1} &\textbf{6.3} &\textbf{6.5} &\textbf{6.5} &\textbf{6.5} &\textbf{3.3} &\textbf{3.0} &\textbf{4.4} &\textbf{4.4} &\textbf{4.7} &\textbf{4.5} \\
\hline
CV2$^{\dag}$&1.0 &1.5 &1.6 &1.6 &1.6 &1.6 &1.9 &1.2 &1.1 &1.1 &1.0 &1.1 \\
CV2$^{*}$&\textbf{2.5} &\textbf{6.3} &\textbf{6.2} &\textbf{6.7} &\textbf{6.3} &\textbf{6.6} &\textbf{4.8} &\textbf{4.3} &\textbf{6.6} &\textbf{7.9} &\textbf{8.3} &\textbf{8.6} \\
\hline
CV3$^{\dag}$&1.2 &1.5 &1.7 &1.8 &1.9 &2.0 &1.4 &1.6 &1.4 &1.3 &1.3 &1.3 \\
CV3$^{*}$&\textbf{2.4} &\textbf{4.9} &\textbf{6.2} &\textbf{6.9} &\textbf{7.2} &\textbf{7.5} &\textbf{4.0} &\textbf{3.1} &\textbf{3.4} &\textbf{7.1} &\textbf{7.4} &\textbf{8.3} \\
\hline
CV4$^{\dag}$&1.2 &2.0 &2.2 &2.3 &2.5 &2.5 &2.4 &1.6 &1.5 &1.4 &1.3 &1.3 \\
CV4$^{*}$&\textbf{2.9} &\textbf{6.7} &\textbf{8.1} &\textbf{9.3} &\textbf{9.1} &\textbf{9.0} &\textbf{3.2} &\textbf{4.6} &\textbf{5.3} &\textbf{6.9} &\textbf{6.3} &\textbf{6.1} \\
\hline
CV5$^{\dag}$&1.2 &2.1 &2.7 &3.6 &4.1 &4.6 &3.1 &2.3 &2.6 &2.5 &2.2 &2.0 \\
CV5$^{*}$&\textbf{2.8} &\textbf{6.5} &\textbf{8.1} &\textbf{12.7} &\textbf{15.6} &\textbf{18.7} &\textbf{3.6} &\textbf{4.7} &\textbf{7.7} &\textbf{3.9} &\textbf{8.3} &\textbf{9.0} \\
\hline
CV6$^{\dag}$&1.2 &1.8 &2.5 &3.2 &3.6 &3.8 &2.8 &1.8 &1.9 &2.0 &1.9 &1.8 \\
CV6$^{*}$&\textbf{2.7} &\textbf{5.8} &\textbf{8.5} &\textbf{11.3} &\textbf{12.6} &\textbf{16.4} &\textbf{3.8} &\textbf{5.1} &\textbf{6.6} &\textbf{4.5} &\textbf{6.0} &\textbf{9.6} \\
\hline
CV7$^{\dag}$&1.1 &1.8 &2.4 &2.6 &2.9 &3.1 &1.6 &1.9 &1.5 &1.6 &1.5 &1.5 \\
CV7$^{*}$&\textbf{2.6} &\textbf{6.0} &\textbf{8.3} &\textbf{9.3} &\textbf{11.6} &\textbf{12.0} &\textbf{3.6} &\textbf{5.5} &\textbf{5.5} &\textbf{5.3} &\textbf{7.3} &\textbf{7.9} \\
\hline
CV8$^{\dag}$&1.2 &1.7 &2.1 &2.2 &2.4 &2.5 &3.8 &1.7 &1.6 &1.4 &1.4 &1.2 \\
CV8$^{*}$&\textbf{2.9} &\textbf{5.9} &\textbf{7.4} &\textbf{8.9} &\textbf{9.9} &\textbf{9.6} &\textbf{8.6} &\textbf{5.0} &\textbf{3.1} &\textbf{6.3} &\textbf{7.1} &\textbf{7.8} \\
\hline
CV9$^{\dag}$&1.2 &1.9 &2.5 &3.0 &3.5 &3.8 &2.7 &2.0 &2.3 &2.1 &2.1 &2.1 \\
CV9$^{*}$&\textbf{3.1} &\textbf{5.5} &\textbf{8.1} &\textbf{10.4} &\textbf{12.7} &\textbf{14.4} &\textbf{4.3} &\textbf{4.7} &\textbf{6.6} &\textbf{5.1} &\textbf{7.9} &\textbf{9.1} \\

\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}

%\begin{figure*}
%\centering
%	
%\subfloat[][$3\times 3$ filter]{\includegraphics[width=\columnwidth]{./figure/depthwise_speedup_3f.eps}
%	\label{fig:f3dwruntime2080}}
%\subfloat[][$5\times 5$ filter]{\includegraphics[width=\columnwidth]{./figure/depthwise_speedup_5f.eps}
%	\label{fig:f5dwruntime2080}}
%
%\vspace{-2mm} \caption{Speedups of our approach and cuDNN implicit algorithm over the baseline implementation of the depthwise convolution when using a
%$3\times 3$ (a) and a $5\times 5$ filter (b).} \label{fig:dwruntime}
%\vspace{-2mm}
%\end{figure*}


\subsubsection{Setup} In this experiment, we compare our approach against the depthwise convolution implementations of cuDNN. Specifically, we compare to seven algorithms in cuDNN, including IMPLICIT\_GEMM (implicit), IMPLICIT\_PRECOMP\_GEMM (precomp), GEMM (gemm), FFT (fft), FFT\_TILING (tiling), WINOGRAD (winograd) and WINOGRAD\_NONFUSED (nonfused). 
As the fastest algorithm of cuDNN is always implicit, we only report the performance of the implicit algorithm for cuDNN. 
We implement a simple depthwise convolution as a baseline implementation. 
We use the convolutional layer configurations that apply depthwise convolution from the popular CNN model, MobileNetv2. 
We report the performance when batch sizes are set to 1, 8, 16, 32, 64 and 128. 
Table \ref{tab:depconvconfigs} gives the layer configurations used in this experiment.

%In the depth-wise convolution, one input element only needs to be convolved with one filter, while in the multi-channel 2D convolution, one input element needs to be convolved with all filters. 
%Therefore, depth-wise convolutions require much less computation than multi-channel 2D convolutions, which makes depth-wise convolutions more sensitive to memory performance.
%Nowadays, depth-wise convolutions have been widely used in embedded CNNs, including MobileNetv2 \cite{Sandler_2018_CVPR}, EfficientNet \cite{tan2019efficientnet} and ShuffleNetv2 \cite{Ma_2018_ECCV}. 
%In this section, we present the performance comparison of the depth-wise convolution between cuDNN and our implementation. 
%We implement a simple depth-wise convolution and report speedups of cuDNN and our implementation over the simple depth-wise convolution. 
%We evaluate 7 algorithms in cuDNN, including IMPLICIT\_GEMM (implicit), IMPLICIT\_PRECOMP\_GEMM (precomp), GEMM (gemm), FFT (fft), FFT\_TILING (tiling), WINOGRAD (winograd) and WINOGRAD\_NONFUSED (nonfused).
%Winograd can not be applied on a $5 \times 5$ filter, thus we set speedups for this algorithm to 0.
%We collect configurations of the convolutional layers using depth-wise convolution from three popular mobile embedded models,
%namely, MobileNetv2, ShuffleNetv2 and EfficientNet.
%Then, we set the number of the batch size to 512 ($I_N=O_N=512$). Other batch sizes demonstrate a similar performance because all tested implementations have a linear scale as the batch size. The exact configuration is presented in Table \ref{tab:3dconvconfigs}.

\subsubsection{Overall results}
Table \ref{tab:dwconvf3} and \ref{tab:dwconvf5} show that our approach gives the best speedups. 
Our approach delivers an average speedup of $5.4\times$ and $7.8\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively. 
The implicit algorithm achieves an average speedup of $2.8\times$ and $2.1\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively.
Other algorithms of cuDNN perform poorly in all test cases. 
Since that cuDNN is a closed source, we cannot investigate the source code for further analysis. 
We hypothesize that FFT- and Winograd- based algorithms focus on the reduction of computation and trades memory performance for speed. 
Precomp and gemm algorithms need extra memory operations to compute output elements. 
Moreover, depthwise convolution is more sensitive to memory performance than pointwise and multi-channel 2D convolutions. 
Consequently, these algorithms are not suitable for depthwise convolutions. 
Overall, our approach improves implicit by $1.9\times$ (up o $3.5\times$) and $3.5\times$ (up to $4.4\times$) when using a $3 \times 3$ and $5 \times 5$ filter respectively.
%When using a $5 \times 5$ filter, our approach achieves a better overall speedup of $7.7\times$. A $5 \times 5$ filter has four overlapped columns and rows (instead of just one
%overlapped column and row on width and height dimensions when using a $3 \times 3$ filter). The larger number of overlapped data thus gives
%more room for performance optimization when using our column and row reuse algorithms. 
%The algorithms of cuDNN except implicit algorithm perform poorly in all test cases and the speedups of these algorithms all below 1.
%Considering that cuDNN is a closed source, we can only guess that FFT- and Winograd- based algorithms focus on reduction of computation and
%trades memory performance for speed. Precomp and gemm algorithms need extra memory operations to compute output elements. Moreover,
%depth-wise convolution is more sensitive to memory performance than multi-channel 2D convolution. Consequently, these algorithms perform
%poorly on depth-wise convolutions.
\begin{figure}
    \centering
    \subfloat[][SM utilizations of cuDNN implicit algorithm and our approach.]{\includegraphics[width=0.9\columnwidth]{./figure/smutil.eps}\label{fig:dwsmutil}}
    \qquad
    \vspace{5mm}
    \subfloat[][Ratios of executed LDG (load from global memory) instruction counts of cuDNN implicit algorithm to our approach.]{\includegraphics[width=0.9\columnwidth]{./figure/ldginst.eps}\label{fig:dwldginst}}
    \vspace{-4mm}
    \caption{SM utilization and LDG instruction counts of depthwise convolutions with a batch size of 32 and a filter size of $3\times3$.}
    \label{fig:dwratio}
\end{figure}

\subsubsection{Further analysis}
We note that our approach is based on the standard 2D direct convolution by applying the column and row reuse algorithms. 
Therefore, the performance gain is mainly attributed to the reduction of the number of memory operations. 

Figure \ref{fig:dwratio} reports the measured LDG (load from global memory) instruction counts and SM utilization for the fast cuDNN implicit algorithm and our approach when using a $3 \times 3$ filter and a batch size of 32. Other configurations have a similar performance. 
We can see in Figure \ref{fig:dwsmutil} that cuDNN implicit algorithm has an average of $2\times$ higher SM utilization compared to our approach.
The reason why our approach gives lower SM utilization is explained as follows. 
Our row reuse algorithm performs better when a thread operates on more rows of output.
However, the more rows a thread compute on, the fewer warps and thread blocks we can generate. Without enough warps running on SMs, the SM utilization will reduce significantly.
Though cuDNN implicit has high SM utilization, it does not achieve good performance for depthwise convolution. 
The reason is that depthwise convolution possesses a very low computational workload and is more sensitive to memory performance.
Different from cuDNN implicit, we focus on optimizing memory performance for depthwise convolution. We use row and column reuse techniques to reduce memory operations by a maximum of $4.5\times$ lower executed LDG instructions, as shown in Figure \ref{fig:dwldginst}.
This suggests that our reuse algorithms can improve memory performance significantly, which thus leads to better overall performance.

\subsubsection{Summary}
Our approach can significantly reduce the number of memory operations and improve memory performance, which leads to faster computation time when performing depthwise convolutions.  
Compared to the fastest available algorithms in cuDNN, which is the state-of-the-art convolution library, our approach achieves an average speedup of $1.9\times$ and $3.5\times$ when using a $3 \times 3$ and $5 \times 5$ filter, respectively.


\subsection{Pointwise Convolution}
\label{sec:pwconvexp}

\begin{table}[]
\caption{Layer configurations of pointwise convolutions$^{\dag}$.}
\label{tab:pwconv}
\rowcolors{2}{}{Gray}
\begin{threeparttable}
\begin{tabular}{lrrrrr}
\toprule
& \textbf{$I_N$} & \textbf{$I_C=F_C$} & \textbf{$I_H \times I_W$} & \textbf{$F_N \times F_H \times F_W$} \\
\midrule
\textbf{CV1}  & 1,8,16,32,64,128  & 16    & $56\times 56$   & $8 \times 1\times 1$\\
\textbf{CV2}  & 1,8,16,32,64,128  & 8     & $56\times 56$   & $16 \times 1\times 1$\\
%\textbf{CONV3}  & 1,8,16,32,64,128  & 16    & $56\times 56$   & 16  & $1\times 1$\\
\textbf{CV3}  & 1,8,16,32,64,128  & 16    & $56\times 56$   & $72 \times 1\times 1$\\
\textbf{CV4}  & 1,8,16,32,64,128  & 72    & $28\times 28$   & $24 \times 1\times 1$\\
%\textbf{CONV6}  & 1,8,16,32,64,128  & 24    & $28\times 28$   & 88  & $1\times 1$\\
%\textbf{CONV7}  & 1,8,16,32,64,128  & 88    & $28\times 28$   & 24  & $1\times 1$\\
\textbf{CV5}  & 1,8,16,32,64,128  & 24    & $28\times 28$   & $96 \times 1\times 1$\\
\textbf{CV6}  & 1,8,16,32,64,128  & 96    & $14\times 14$   & $24 \times 1\times 1$\\
\textbf{CV7} & 1,8,16,32,64,128  & 24    & $14\times 14$   & $96  \times 1\times 1$\\
%\textbf{CONV11} & 1,8,16,32,64,128  & 96    & $14\times 14$   & 32  & $1\times 1$\\
\textbf{CV8} & 1,8,16,32,64,128  & 32    & $14\times 14$   & $192 \times 1\times 1$\\
\textbf{CV9} & 1,8,16,32,64,128  & 192   & $14\times 14$   & $48  \times 1\times 1$\\
%\textbf{CONV14} & 1,8,16,32,64,128  & 48    & $14\times 14$   & 192 & $1\times 1$\\
%\textbf{CONV15} & 1,8,16,32,64,128  & 192   & $14\times 14$   & 32  & $1\times 1$\\
%\textbf{CONV16} & 1,8,16,32,64,128  & 32    & $14\times 14$   & 96  & $1\times 1$\\
\textbf{CV10} & 1,8,16,32,64,128  & 96    & $14\times 14$   & $40 \times 1\times 1$\\
\textbf{CV11} & 1,8,16,32,64,128  & 40    & $14\times 14$   & $120 \times 1\times 1$\\
\textbf{CV12} & 1,8,16,32,64,128  & 120   & $14\times 14$   & $32 \times 1\times 1$\\
%\textbf{CONV20} & 1,8,16,32,64,128  & 32    & $14\times 14$   & 120 & $1\times 1$\\
%\textbf{CONV21} & 1,8,16,32,64,128  & 120   & $14\times 14$   & 40  & $1\times 1$\\
\textbf{CV13} & 1,8,16,32,64,128  & 40    & $14\times 14$   & $240 \times 1\times 1$\\
\textbf{CV14} & 1,8,16,32,64,128  & 240   & $7\times 7$     & $64  \times 1\times 1$\\
\textbf{CV15} & 1,8,16,32,64,128  & 64    & $7\times 7$     & $240 \times 1\times 1$\\
%\textbf{CONV25} & 1,8,16,32,64,128  & 240   & $7\times 7$     & 72  & $1\times 1$\\
\textbf{CV16} & 1,8,16,32,64,128  & 72    & $7\times 7$     & $432 \times 1\times 1$\\
\textbf{CV17} & 1,8,16,32,64,128  & 432   & $7\times 7$     & $112 \times 1\times 1$\\
\textbf{CV18} & 1,8,16,32,64,128  & 112   & $7\times 7$     & $432 \times 1\times 1$\\
\textbf{CV19} & 1,8,16,32,64,128  & 432   & $7\times 7$     & $72 \times 1\times 1$\\
\textbf{CV20} & 1,8,16,32,64,128  & 432   & $7\times 7$     & $1024 \times 1\times 1$\\

\bottomrule
\end{tabular}
\begin{tablenotes}
\item[\dag] We use $I$, $F$, and $O$ to represent the input, the filter, and the output respectively, $N$, $C$, $H$, and $W$
to denote the batch size, the channel, the height, and the width, respectively.
\end{tablenotes}
\end{threeparttable}
\end{table}


\begin{table}[]
\setlength{\tabcolsep}{3.8pt}
\caption{Layer configurations of pointwise convolutions$^{\dag}$.}
\label{tab:pwconv}
%\rowcolors{2}{}{Gray}
\begin{threeparttable}
\begin{tabular}{c|cccccc|cccccc}
\toprule
&\multicolumn{6}{c|}{Speedups on RTX2080Ti} & \multicolumn{6}{c}{Speedups on Jetson Xavier}\\


\midrule
Batch Size& \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128} & \textbf{1} & \textbf{8} & \textbf{16}& \textbf{32} & \textbf{64} & \textbf{128}\\
\hline
CV1$^{\dag}$&1.2 &1.2 &1.3 &1.1 &1.2 &1.4 &1.5 &1.0 &1.0 &1.0 &1.3 &1.5 \\
CV1$^{*}$&\textbf{4.2} &\textbf{4.1} &\textbf{3.6} &\textbf{2.9} &\textbf{2.6} &\textbf{2.4} &\textbf{3.3} &\textbf{2.3} &\textbf{3.0} &\textbf{2.1} &\textbf{2.4} &\textbf{2.9} \\
\hline
CV2$^{\dag}$&1.3 &1.2 &1.3 &1.3 &1.4 &1.4 &2.0 &1.3 &1.2 &1.6 &1.6 &2.0 \\
CV2$^{*}$&\textbf{4.3} &\textbf{3.7} &\textbf{3.2} &\textbf{2.8} &\textbf{2.3} &\textbf{2.0} &\textbf{3.7} &\textbf{3.0} &\textbf{2.2} &\textbf{2.8} &\textbf{2.4} &\textbf{2.8} \\
\hline
CV3$^{\dag}$&1.3 &1.1 &1.2 &1.1 &1.2 &1.2 &1.2 &1.5 &1.5 &1.6 &1.6 &1.9 \\
CV3$^{*}$&\textbf{3.4} &\textbf{2.2} &\textbf{1.8} &\textbf{1.6} &\textbf{1.6} &\textbf{1.6} &\textbf{3.3} &\textbf{2.5} &\textbf{2.2} &\textbf{2.2} &\textbf{2.2} &\textbf{2.5} \\
\hline
CV4$^{\dag}$&1.1 &1.1 &1.2 &1.6 &1.6 &1.9 &1.0 &1.2 &1.2 &1.4 &1.7 &2.0 \\
CV4$^{*}$&\textbf{3.3} &\textbf{2.9} &\textbf{2.4} &\textbf{2.1} &\textbf{1.7} &1.8 &\textbf{2.2} &\textbf{1.7} &\textbf{1.5} &\textbf{1.5} &1.4 &1.3 \\
\hline
CV5$^{\dag}$&1.2 &1.4 &1.5 &1.3 &1.3 &1.1 &1.3 &1.3 &1.7 &1.3 &1.8 &2.1 \\
CV5$^{*}$&\textbf{3.7} &\textbf{3.3} &\textbf{2.7} &\textbf{2.1} &\textbf{1.8} &\textbf{1.6} &\textbf{2.4} &\textbf{2.3} &\textbf{2.2} &\textbf{1.9} &1.7 &1.7 \\
\hline
CV6$^{\dag}$&1.1 &1.2 &1.2 &1.2 &1.4 &1.7 &1.0 &1.4 &1.3 &1.3 &1.3 &1.2 \\
CV6$^{*}$&\textbf{4.1} &\textbf{3.8} &\textbf{3.5} &\textbf{2.7} &\textbf{2.5} &\textbf{1.9} &\textbf{3.6} &\textbf{3.3} &\textbf{2.3} &\textbf{2.1} &\textbf{2.0} &\textbf{1.8} \\
\hline
CV7$^{\dag}$&1.2 &1.2 &1.3 &1.3 &1.5 &1.4 &1.5 &1.4 &1.3 &1.6 &1.3 &1.9 \\
CV7$^{*}$&\textbf{3.5} &\textbf{3.4} &\textbf{3.4} &\textbf{3.2} &\textbf{2.9} &\textbf{2.2} &\textbf{2.4} &\textbf{2.7} &\textbf{2.6} &\textbf{2.3} &\textbf{1.5} &\textbf{2.2} \\
\hline
CV8$^{\dag}$&1.2 &1.3 &1.3 &1.6 &1.5 &1.6 &1.3 &1.3 &1.6 &1.7 &1.9 &2.4 \\
CV8$^{*}$&\textbf{2.9} &\textbf{2.7} &\textbf{2.7} &\textbf{2.6} &\textbf{2.4} &\textbf{2.3} &\textbf{2.1} &\textbf{2.2} &\textbf{1.9} &\textbf{2.6} &\textbf{2.6} &2.3 \\
\hline
CV9$^{\dag}$&1.7 &1.7 &1.7 &2.3 &2.3 &2.4 &0.8 &1.0 &1.3 &1.2 &1.5 &1.7 \\
CV9$^{*}$&\textbf{4.5} &\textbf{4.8} &\textbf{4.2} &\textbf{4.2} &\textbf{3.2} &\textbf{2.5} &\textbf{2.9} &\textbf{2.1} &\textbf{2.1} &\textbf{1.7} &\textbf{1.6} &1.5 \\
\hline
CV10$^{\dag}$&1.2 &1.2 &1.2 &1.2 &1.6 &1.6 &0.9 &1.5 &1.1 &1.2 &1.2 &1.3 \\
CV10$^{*}$&\textbf{2.8} &\textbf{3.7} &\textbf{3.0} &\textbf{2.8} &\textbf{2.7} &\textbf{2.1} &\textbf{3.3} &\textbf{3.2} &\textbf{2.2} &\textbf{1.7} &\textbf{1.9} &\textbf{1.7} \\
\hline
CV11$^{\dag}$&1.2 &1.2 &1.4 &1.4 &1.6 &1.5 &1.4 &1.3 &1.4 &1.2 &1.2 &1.7 \\
CV11$^{*}$&\textbf{2.9} &\textbf{2.7} &\textbf{3.2} &\textbf{2.5} &\textbf{2.1} &\textbf{1.8} &\textbf{2.5} &\textbf{2.0} &\textbf{2.0} &\textbf{1.8} &\textbf{1.7} &1.5 \\
\hline
CV12$^{\dag}$&1.3 &1.3 &1.3 &1.5 &1.6 &1.9 &1.2 &1.2 &1.3 &1.3 &1.3 &1.2 \\
CV12$^{*}$&\textbf{3.9} &\textbf{3.9} &\textbf{3.6} &\textbf{3.4} &\textbf{2.8} &\textbf{2.1} &\textbf{3.0} &\textbf{2.8} &\textbf{2.0} &\textbf{1.6} &\textbf{1.8} &\textbf{1.6} \\
\hline
CV13$^{\dag}$&1.2 &1.4 &1.3 &1.5 &1.5 &1.5 &2.0 &1.4 &1.4 &1.6 &1.7 &2.1 \\
CV13$^{*}$&\textbf{3.2} &\textbf{2.8} &\textbf{2.5} &\textbf{2.3} &\textbf{1.9} &\textbf{1.7} &\textbf{3.4} &\textbf{2.3} &\textbf{2.0} &\textbf{1.8} &1.7 &1.6 \\
\hline
CV14$^{\dag}$&2.0 &2.1 &2.0 &1.9 &2.1 &2.6 &1.3 &1.3 &1.5 &1.4 &1.3 &1.5 \\
CV14$^{*}$&\textbf{7.4} &\textbf{6.2} &\textbf{5.0} &\textbf{3.7} &\textbf{3.6} &\textbf{2.9} &\textbf{1.4} &\textbf{2.9} &1.5 &\textbf{2.1} &\textbf{1.5} &1.2 \\
\hline
CV15$^{\dag}$&1.1 &1.2 &1.2 &1.4 &1.4 &1.5 &1.2 &1.3 &1.2 &1.2 &1.2 &1.1 \\
CV15$^{*}$&\textbf{3.7} &\textbf{2.8} &\textbf{2.5} &\textbf{2.6} &\textbf{2.2} &\textbf{1.7} &\textbf{3.1} &\textbf{2.3} &\textbf{2.0} &\textbf{1.8} &\textbf{1.8} &0.8 \\
\hline
CV16$^{\dag}$&1.2 &1.3 &1.2 &1.4 &1.7 &1.6 &1.3 &1.4 &1.2 &1.5 &1.5 &1.3 \\
CV16$^{*}$&\textbf{4.3} &\textbf{3.5} &\textbf{2.7} &\textbf{2.1} &\textbf{2.3} &\textbf{1.9} &0.9 &\textbf{2.0} &\textbf{1.9} &1.5 &1.2 &\textbf{1.4} \\
\hline
CV17$^{\dag}$&1.8 &2.0 &1.7 &1.7 &2.4 &2.2 &1.2 &1.3 &1.4 &1.3 &1.4 &1.4 \\
CV17$^{*}$&\textbf{7.2} &\textbf{4.7} &\textbf{3.4} &\textbf{3.4} &\textbf{3.3} &\textbf{2.3} &\textbf{3.1} &1.3 &\textbf{1.6} &1.3 &1.3 &1.4 \\
\hline
CV18$^{\dag}$&1.2 &1.3 &1.4 &1.3 &1.9 &1.8 &1.0 &1.1 &1.2 &1.2 &1.4 &1.5 \\
CV18$^{*}$&\textbf{3.7} &\textbf{3.2} &\textbf{2.6} &\textbf{1.9} &\textbf{2.3} &1.8 &\textbf{3.9} &\textbf{1.9} &\textbf{2.0} &\textbf{1.6} &1.4 &1.3 \\
\hline
CV19$^{\dag}$&1.8 &2.0 &1.6 &1.7 &2.2 &2.0 &1.2 &1.5 &1.4 &1.2 &1.2 &1.2 \\
CV19$^{*}$&\textbf{7.6} &\textbf{5.6} &\textbf{4.5} &\textbf{4.0} &\textbf{3.6} &\textbf{2.7} &\textbf{4.2} &\textbf{1.6} &\textbf{2.1} &\textbf{1.4} &\textbf{1.5} &\textbf{1.4} \\
\hline
CV20$^{\dag}$&1.7 &1.5 &2.0 &1.7 &2.0 &1.8 &1.2 &1.1 &1.6 &1.1 &1.2 &1.3 \\
CV20$^{*}$&0.5 &0.7 &1.0 &1.2 &1.1 &1.1 &0.8 &0.7 &1.2 &0.9 &0.8 &1.0 \\

\bottomrule
\end{tabular}
\end{threeparttable}
\end{table}




%\begin{figure}
%\centering
%	
%\subfloat{\includegraphics[width=\columnwidth,height=13cm]{./figure/pwexectime.eps}}
%\vspace{-2mm}
%\caption{Speedups of cuDNN implicit and our approach over cuDNN gemm} \label{fig:pwexectime}
%\vspace{-5mm}
%\end{figure}


\subsubsection{Setup} In this experiment, we compare our approach against the pointwise convolution implementations in cuDNN and use cuDNN gemm algorithm as the baseline. 
We use the layer configurations from the popular CNN model MobileNetv2.
There are 30 different configurations for pointwise convolution, we test all configurations and report the performance of 20 layers. 
The other 10 layers exhibit similar performance as the selected ones.  
We report the performance when batch sizes are set to 1, 8, 16, 32, 64 and 128.
In all configurations of pointwise convolution, cuDNN implicit performs best among all algorithms in cuDNN, therefore we only show results of cuDNN implicit.
Table \ref{tab:pwconv} lists the layer configurations used in this experiment.

\subsubsection{Overall results} Figure \ref{fig:pwexectime} shows the speedups of cuDNN implicit and our approach over cuDNN gemm.
Our approach achieves the best performance for layers of CV1-CV19.
For the layer CV20, which uses 1024 filters to convolve with the input, our approach performs poorly compared to cuDNN gemm and cuDNN implicit.
Overall, our implementation and cuDNN implicit achieve an average speedup of $2.8\times$ and $1.5\times$ over cuDNN gemm. 
This translates to an improvement of $2\times$ over cuDNN implicit, which is the fastest algorithm in cuDNN for pointwise convolution.

\subsubsection{Further analysis}
\begin{figure}
    \centering
    \subfloat[][SM utilizations of cuDNN implicit and our approach.]{\includegraphics[width=\columnwidth]{./figure/pwsmutil.eps}\label{fig:pwsmutil}}
    \qquad
    \vspace{5mm}
    \subfloat[][Ratios of executed LDG (load from global memory) instruction counts of cuDNN implicit to our approach.]{\includegraphics[width=\columnwidth]{./figure/pwldginst.eps}\label{fig:pwldginst}}
    \vspace{-4mm}
    \caption{SM utilizations and ratios of executed LDG instruction counts for pointwise convolutions with a batch size of 32.}
    \label{fig:pwinfo}
\end{figure}
Figure \ref{fig:pwinfo} reports the measured SM utilizations and ratios of executed LDG instruction counts of cuDNN implicit to our approach when using a batch size of 32. Other configurations have the similar performance. 

Given a specific layer configuration of pointwise convolution, our approach will try to create a suitable number of thread blocks to saturate GPU. 
A potential performance issue is that more thread blocks may incur more reloads of filters or inputs which are shared between thread blocks. 
As shown in Figure \ref{fig:pwldginst}, our approach needs more than $2\times$ LDG instructions than cuDNN implicit in some cases.
In contrary to memory performance, the overall performance of our approach is $2\times$ faster than cuDNN implicit.
There are two reasons for the performance gain of our approach. 
\begin{enumerate}
	\item Our approach exhibits a much higher SM utilization than cuDNN implicit.
	We can see in Figure \ref{fig:pwsmutil} that our approach has an average of $1.9\times$ higher SM utilization compared to cuDNN implicit.
	When executing pointwise convolution with a batch size of 128 or below, cuDNN implicit can not utilize SMs efficiently because of its fixed block size. 
	Based on our dynamic block size, we improve SM utilization which leads to performance improvement for pointwise convolution.
	\item We use a double buffer and multiple accumulations of output to hide memory access latency.
	Figure \ref{fig:stalllongscore} shows the average number of cycles each warp spends on waiting for the global operation to complete. 
	We implement a simple pointwise without latency hiding, denoted as simple. 
	We can see that our approach can significantly reduce the memory access latency compared to the simple implementation.
	To further reduce memory access latency, we need assembly level optimizaitions which is not the focus of this work.
\subsubsection{Summary} We use dynamic block size to improve SM utilization and use a double buffer and multiple accumulations to hide memory access latency. With both methods, we achieve an average speedup of $2\times$ over cuDNN implicit.
\end{enumerate}

\begin{figure*}
    \centering
    \includegraphics[width=0.97\textwidth,height=4.5cm]{./figure/longscore.eps}
    \caption{The average number of cycles each warp spends on waiting for the global operation to complete.}
    \label{fig:stalllongscore}
\end{figure*}

\subsection{Inference Performance}
\label{sec:inferexp}
\subsubsection{Setup} In this experiment, we apply our depthwise and pointwise convolutions on the inference of MobileNetv2 for batch sizes of 1, 8, 16, 32, 64 and 128.
We use the caffe framework to implement MobileNetv2 and made some modifications to the source code of caffe before performing experiments. 
First, we replace the implementation of batchnormalization and depthwise convolution layers with cuDNN implementations. 
To experiment, we first run the inference with cuDNN implementation and then replace the cuDNN with our implementation.

\subsubsection{Overall result}
\begin{table}[]
\setlength{\tabcolsep}{3.4pt}
    \caption{Experiments for inference using MobileNetv2}
    \label{tab:infertime}
    \centering
    %\rowcolors{2}{}{Gray}
    \begin{threeparttable}
    \begin{tabular}{c|l|rrrrrr}
    \toprule
    &\textbf{batch size} & 1 & 8 & 16& 32 &64 & 128\\
    \midrule
    \multirow{3}{*}{\textbf{RTX2080Ti}}&\textbf{cuDNN (ms)} & 7.5  & 8.8 & 9.7 & 14.4 & 19.1 &28.7 \\
    &\textbf{Ours (ms)} &6.1   &7.1    & 8.0  &12.0 &16.9 &26.3\\
    &\textbf{Improvement (\%)} &18.6   &19.3    &17.5  & 16.7 &11.5 &8.4 \\
    \hline
    \multirow{3}{*}{\textbf{AGX Xavier}}&\textbf{cuDNN (ms)} & 16.6&22.3&32.1&52.6&84.2&140.1  \\
    &\textbf{Ours (ms)} &13.2&18.9&27.8&44.7&76.1&130.0 \\
    &\textbf{Improvement (\%)} & 20.5&15.2&13.4&15.0&9.6&7.2 \\
    \bottomrule
    \end{tabular}
    \footnotesize
    \end{threeparttable}
    \vspace{-5mm}
\end{table}

\begin{table}[]
    \caption{Experiments for inference using MobileNetv2}
    \label{tab:infertime}
    \centering
    %\rowcolors{2}{}{Gray}
    \begin{threeparttable}
    \begin{tabular}{c|l|rrrr}
    \toprule
    &\textbf{batch size} & 16& 32 &64 & 128\\
    \midrule
    \multirow{3}{*}{\textbf{RTX2080Ti}}&\textbf{cuDNN (ms)} & 16.6 & 27.6 & 43,4 &75.36 \\
    &\textbf{Ours (ms)} & 14.5  &24.1 &39.9 &71.28\\
    &\textbf{Improvement (\%)} &12.7  &12.7 &8.1 &5.4 \\
    \bottomrule
    \end{tabular}
    \footnotesize
    \end{threeparttable}
    \vspace{-5mm}
\end{table}
Table \ref{tab:infertime} demonstrates that our approach improves the performance of inference by 12.19\% on average compared to the implementation of cuDNN implicit.