

\section{Experimental Setup}

\subsection{Evaluation Platforms} We evaluate our approach on and NVIDIA RTX 2080Ti GPU, which integrates 4350 CUDA cores for floating
point computation  and 4350 CUDA core for integer operations. The GPU has a 96KB L1 cache and shared memory. The host machine has a 2.30GHz
Intel Xeon E5-2697 CPU with 252GB memory, running Linux kernel v4.15.0. We use CUDA Toolkit version 10.2.


\subsection{Competing Methods} We compare our approach against the following state-of-the-art image and convolution libraries:
\begin{itemize}
  \item \textbf{cuDNN version 7.6.4}. cuDNN is a state-of-the-art convolution implementation that supports 2D and depth-wise convolutions
      on GPU. Moreover, cuDNN can execute GEMM-, FFT- and Winograd-based convolutions.
  \item \textbf{ArrayFire \cite{Yalamanchili2015}, version 3.6.4}. ArrayFire is a popular image and signal processing library. This
      library implements 2D convolutions on GPU. ArrayFire uses Just In Time compiling for standard arithmetic operations. Thus, the
      first run of an ArrayFire application takes longer than the second run. In the experiment, we run ArrayFire twice in each test and
      record the second runtime.
  \item N\textbf{VIDIA Performance Primitives (NPP)}. This is an image and signal processing library. We use NPP for 2D convolutions
      only.
  \item \textbf{GEMM-im2col (im2col)}. We extract the implementation of the im2col from Caffe \cite{jia2014caffe} and take it as a
      baseline for the 2D and multi-channel 2D convolutions.
  \item Direct implementation of depth-wise convolution. We implement a direct depth-wise convolution without using the proposed reuse algorithms. We take this implementation as a baseline for depth-wise convolution.

\end{itemize}

\subsection{Use Cases}
We apply our approach to three representative convolution operations, single-channel 2D convolution, depth-wise convolution and
multi-channel 2D convolution, described as follows.

\mypara{Single-channel 2D convolution.} \FIXME{briefly explain the method.}

\mypara{Depth-wise convolution.} \FIXME{briefly explain the method.}

\mypara{Multi-channel 2D convolution.} \FIXME{briefly explain the method.}


\subsection{Performance Report} We run each test case ten times on an unloaded machine and report the averaged running time. We found
little variance during execution runs, less than 2\%.  The data type is 32-bit (single-precision) floating point, and all the data are
organized as 4D tensors $(N,C,H,$ and $W)$. In this work, we test 2D, multi-channel 2D and depth-wise convolutions with two filters sizes,
$3 \times 3$ and $5 \times 5$, because these are commonly used filter sizes.

\section{Experimental Results}
\label{exp} In this section, we report our results 2D convolution (Section \ref{sec:ex2dc}), depth-wise convolution (Section
\ref{sec:depconvexp}) and multi-channel 2D convolution (Section \ref {multicconvexp}), showing that our approach consistently outperforms
alternative methods for all use cases.


\subsection{2D Convolution\label{sec:ex2dc}}
\begin{figure*}
\centering
\subfloat[Speedups for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/2d_conv_f3.eps}
	\label{fig:2druntimef3c12080}}
\hspace{0em}
\subfloat[Speedups for the filter of size $5 \times 5$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/2d_conv_f5.eps}
	\label{fig:2druntimef5c12080}}
	
\caption{Speedups of 2D convolutions of four implementations over im2col.}
\label{fig:2druntime}
\end{figure*}



In this experiment, we compare our approach against the 2D convolution implementation obtained from cuDNN, im2col,  ArrayFire, and NPP. We
apply each metho to image size ($I_H \times I_W$) ranging from $256 \times 256$ to $4K \times 4K$, where $I_N=F_N=1$ and $I_C=F_C=1$.

Figure \ref{fig:2druntime} presents the speedups of cuDNN, ArrayFire, NPP and our implementation over im2col. The results show that cuDNN is not suitable for 2D convolutions in contrast to ArrayFire, NPP and the proposed method. The average speedups of cuDNN, ArrayFire, NPP and ours over im2col are 1.09$\times$, 2.2$\times$, 4.9$\times$ and 6.5$\times$, respectively. Our implementation exhibits superior performance over other implementations. Our implementation of the 2D convolution is derived from direct convolution. We use column (Algorithm \ref{algo:basic}, Algorithm \ref{algo:basic2}) and row reuse (Algorithm \ref{algo:rowreuse}) on direct convolution. Hence, the performance gain is mainly attributed to the reduction on the number of memory transactions.

The proposed implementation achieves the best speedups in six out of ten test cases. NPP achieves the best results on small image sizes, whereas our implementation demonstrates the best results on large image sizes. We use \emph{nvidia compute} to collect memory profiles, including memory throughput and max bandwidth, of our implementation and NPP when convolving with a $3 \times 3$ filter. The results are shown in Figure \ref{fig:2dmemanaly}.

We can see that as the input size increases, memory throughput and max bandwidth of our implementation exceeds that of NPP, the turning point ($1K \times 1K$ in Figure \ref{fig:2dmemanaly}) is the same as that of speedups in Figure \ref{fig:2druntimef3c12080}, which means that memory performance has a strong relationship with the performance of 2D convolutions. Next, we give a detailed analysis of why the memory throughput and max bandwidth of our implementation is low for small input sizes.

When performing 2D convolutions, only one filter is used to convolve with one single-channel input feature map, which requires much less computation than depth-wise convolutions. Therefore, the memory performance has a huge impact on the performance of 2D convolutions. The proposed row reuse algorithm performs better when a thread calculates more rows of output. However, the more rows each thread calculates, the less warps and thread blocks we can generate. Without enough warps issuing memory requests, the memory throughput and max bandwidth can reduce a lot, which can slow down our implementations. As the input size increases, we can allocate more thread blocks and more warps per thread block. With enough memory requests and reduction on redundant memory transactions, we can increase the memory throughput and max bandwidth, and thus improve the performance of our implementation.

The average speedup of our implementation for $5 \times 5$ filter is $7.7\times$, which is better than the speedup of $5.4\times$ for $3 \times 3$ filter. The key reason for the improvement is that, for $3 \times 3$ filter, there is only one overlapped column and row on width and height dimensions. While for $5 \times 5$ filter, there are four overlapped columns and rows, therefore column and row reuse algorithms can be used more efficiently compared with  that for the $3 \times 3$ filter.


\begin{figure}
\centering

\subfloat[Memory throughput for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/2dmemthroughput.eps}
	\label{fig:2dmemthr}}
\hspace{0em}
\subfloat[Max bandwidth for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/2dmembandwidth.eps}
	\label{fig:2dmaxband}}
	
\caption{Memory analysis of NPP and our implementation for the filter of size $3 \times 3$.}
\label{fig:2dmemanaly}
\end{figure}


In summary, our optimization algorithms can greatly reduce the number of memory transactions and improve the performance of 2D
convolutions. Compared with state-of-the-art image processing libraries, NPP, our implementation achieve average speedups of 1.3$\times$ and 1.28$\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively.

\subsection{Depth-wise Convolution}
\label{sec:depconvexp}

\begin{table}[]
\caption{Configurations of 3D convolution}
\label{tab:3dconvconfigs}
\centering
\begin{tabular}{c|ccccc}
\hline
& $I_N$ & $I_C$ & $I_H \times I_W$ &  $F_H \times F_W$ \\
\hline
CONV1 & 512  & 32    & 112*112 & $3 \times 3$, $5 \times 5$  \\
CONV2 & 512  & 96    & 112*112  &$3 \times 3$, $5 \times 5$   \\
CONV3 & 512  & 144   & 56*56  &$3 \times 3$, $5 \times 5$    \\
CONV4 & 512  & 160    & 56*56  &$3 \times 3$, $5 \times 5$    \\
CONV5 & 512  & 192   & 28*28  &$3 \times 3$, $5 \times 5$    \\
CONV6 & 512  & 240   & 28*28  &$3 \times 3$, $5 \times 5$    \\
CONV7 & 512  & 256   & 28*28  &$3 \times 3$, $5 \times 5$    \\
CONV8 & 512  & 384   & 14*14  &$3 \times 3$, $5 \times 5$    \\
CONV9 & 512  & 480   & 14*14  &$3 \times 3$, $5 \times 5$    \\
CONV10 & 512  & 672  & 14*14 &$3 \times 3$, $5 \times 5$     \\
CONV11 & 512  &672  & 7*7 & $3 \times 3$, $5 \times 5$      \\
CONV12 & 512  &960  & 7*7 & $3 \times 3$, $5 \times 5$      \\
CONV13 & 512  &1152  & 7*7 & $3 \times 3$, $5 \times 5$      \\
\hline
\end{tabular}
\end{table}

\begin{figure*}
\centering
	
\subfloat[Speedups for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6.8cm]{./figure/depthwise_f3.eps}
	\label{fig:f33druntime2080}}
\subfloat[Speedups for the filter of size $5 \times 5$.]{\includegraphics[width=\columnwidth,height=6.8cm]{./figure/depthwise_f5.eps}
	\label{fig:f53druntime2080}}

\caption{Speedups of our implementation and seven cuDNN algorithms over the baseline implementation for the depth-wise convolution.}
\label{fig:3druntime}
\end{figure*}


To demonstrate the effectiveness of two reuse algorithms, we also apply Algorithm \ref{algo:basic}, \ref{algo:basic2} and \ref{algo:rowreuse} on the depth-wise convolution. In the depth-wise convolution, one input element only needs to be convolved with one filter, while in the multi-channel 2D convolution, one input element needs to be convolved with all filters. Therefore, depth-wise convolutions require much less computation than multi-channel 2D convolutions, which makes depth-wise convolutions more sensitive to memory performance.


Nowadays, depth-wise convolutions have been widely used in embedded CNNs, including MobileNetv2 \cite{Sandler_2018_CVPR}, EfficientNet \cite{tan2019efficientnet} and ShuffleNetv2 \cite{Ma_2018_ECCV}. In this section, we present the performance comparison of the depth-wise convolution between cuDNN and our implementation. We implement a simple depth-wise convolution and report speedups of cuDNN and our implementation over the simple depth-wise convolution. We evaluate 7 algorithms in cuDNN, including IMPLICIT\_GEMM (implicit), IMPLICIT\_PRECOMP\_GEMM (precomp), GEMM (gemm), FFT (fft), FFT\_TILING (tiling), WINOGRAD (winograd) and WINOGRAD\_NONFUSED (nonfused). Winograd can not be applied on a $5 \times 5$ filter, thus we set speedups for this algorithm to 0.

We collect configurations of the convolutional layers using depth-wise convolution from three popular mobile embedded models,
namely, MobileNetv2, ShuffleNetv2 and EfficientNet.
Then, we set the number of the batch size to 512 ($I_N=O_N=512$). Other batch sizes demonstrate a similar performance because all tested implementations have a linear scale as the batch size. The exact configuration is presented in Table \ref{tab:3dconvconfigs}.

The speedups are shown in Figure \ref{fig:3druntime}. It is obvious that our implementation performs best in all test cases. We obtain an average speedup of $4.5\times$ and $7.6\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively. The implicit algorithm is the fastest algorithm in cuDNN in all test cases. It achieves an average speedup of $2.8\times$ and $1.8\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively. Compared with the fastest algorithm of cuDNN, the proposed approach achieves an average speedup of $1.5\times$, with the maximum speedup reaching $2\times$ for the $3 \times 3$ filter, and an average speedup of $4\times$, with the maximum speedup reaching $4.5\times$ for the $5 \times 5$ filter.

The algorithms of cuDNN except implicit algorithm perform poorly in all test cases and the speedups of these algorithms all below 1. Considering that cuDNN is a closed source, we can only guess that FFT- and Winograd- based algorithms focus on reduction of computation and trades memory performance for speed. Precomp and gemm algorithms need extra memory operations to compute output elements. Moreover, depth-wise convolution is more sensitive to memory performance than multi-channel 2D convolution. Consequently, these algorithms perform poorly on depth-wise convolutions.

\begin{figure}
\centering

\subfloat[Memory throughput for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/depwisememthroughput.eps}
	\label{fig:depwisememthr}}
\hspace{0em}
\subfloat[Max bandwidth for the filter of size $3 \times 3$.]{\includegraphics[width=\columnwidth,height=6cm]{./figure/depwisemembandwidth.eps}
	\label{fig:depwisemaxband}}
	
\caption{Memory analysis of cuDNN implicit algorithm and our implementation for the filter of size $3 \times 3$.}
\label{fig:depwisememanaly}
\end{figure}

Figure \ref{fig:depwisememanaly} demonstrates the memory throughput and max bandwidth for our implementation and cuDNN implicit algorithm when convolving with a $3 \times 3$ filter. We can see that our implementation achieves $1.5 \times$ memory throughput and max bandwidth compared with the implicit algorithm. This demonstrates that our reuse algorithms can improve the memory performance significantly.

In summary, both of reuse algorithms can significantly improve the memory throughput and max bandwidth, which leads to the performance improvements of depth-wise convolutions.  In contrast to the fastest algorithms in cuDNN, which is the state-of-the-art convolution library, our implementation achieves an average speedup of $1.5\times$ and $4\times$ for the $3 \times 3$ and $5 \times 5$ filters, respectively.

\subsection{Multi-channel 2D Convolution}
\label{multicconvexp}

\begin{table}[]
\caption{Configurations of multi-channel 2D convolution}
\label{tab:3dconvconfigs}
\begin{tabular}{c|ccccc}
\hline
& $I_N$ & $I_C=F_C$ & $I_H \times I_W$ & $F_N$ & $F_H \times F_W$ \\
\hline
CONV1 & 128  & 1,3       & $28\times 28$     & 128  & $3\times 3$       \\
CONV2 & 128  & 1,3       & $56\times 56$     & 64   & $3\times 3$       \\
CONV3 & 128  & 1,3       & $12\times 12$     & 64   & $5\times 5$       \\
CONV4 & 128  & 1,3       & $14\times 14$     & 16   & $5 \times 5$       \\
CONV5 & 128  & 1,3       & $24\times 24$    & 256  & $5 \times 5$       \\
CONV6 & 128  & 1,3       & $24\times 24$     & 64   & $5\times 5$       \\
CONV7 & 128  & 1,3       & $28\times 28$     & 16   & $5\times 5$       \\
CONV8 & 128  & 1,3       & $28\times 28$     & 512   & $3\times 3$       \\
CONV9 & 128  & 1,3       & $56\times 56$     & 256  & $3\times 3$       \\
CONV10 & 128  & 1,3       & $112\times 112$     & 128   & $3\times 3$       \\
CONV11 &128  & 1,3       & $224\times 224$     & 64   & $3\times 3$      \\
\hline
\end{tabular}
\end{table}

\begin{figure*}
\centering
	
\subfloat[Speedups for one input channel.]{\includegraphics[width=\columnwidth,height=6.8cm]{./figure/multic2d_c1.eps}
	\label{fig:multic1runtime2080}}
\subfloat[Speedups for three input channels.]{\includegraphics[width=\columnwidth,height=6.8cm]{./figure/multic2d_c3.eps}
	\label{fig:multic3runtime2080}}

\caption{Speedups of our implementation and cuDNN over im2col for one and three input channels.}
\label{fig:multi2druntime}
\end{figure*}


We apply Algorithm \ref{algo:basic}, \ref{algo:basic2} and \ref{algo:rowreuse} on multi-channel 2D convolutions with one and three input channels. The focus of this study is to optimize the memory transactions, and our implementation of multi-channel 2D convolutions do not optimize on input channels. Therefore, our implementation is suitable for convolutions with one and three input channels, which are normally the first layers of a CNN.

In this section, we present the performance comparison of the multi-channel 2D convolutions among three implementations, that is, cuDNN, im2col and our implementation. We collect the configurations of the convolutional layers using $3 \times 3$ and $5 \times 5$ filers from four popular CNN models,
namely, AlexNet \cite{Krizhevsky2012ImageNet}, VGG \cite{SimonyanZ14a}, ResNet \cite{HeZRS16} and GoogleLeNet \cite{SzegedyLJSRAEVR15}.
Then, we set the number of input channels to one and three ($I_C=F_C \in \{1, 3\}$) and the batch size to 128 ($I_N=O_N=128$). The exact configuration is presented in Table \ref{tab:3dconvconfigs}.

The speedups of our implementation and cuDNN over im2col are shown
in Figure \ref{fig:multi2druntime}. Our
implementation achieves average speedups of 19.5$\times$ and 25.6$\times$ relative to im2col for one and three input channels, respectively. Compared with the fastest algorithm in cuDNN, the proposed approach achieves an average speedup of 1.23$\times$ and 1.13$\times$ for one and three input channels, respectively.
%Different from depth-wise convolution, FFT- and Wingorad-based convolutions performs well for multi-channel 2D convolutions. The reason is that, multi-channel 2D convolution is computation intensive and reduction on computation can significantly improve the performance of multi-channel 2D convolution. Both FFT- and Winograd-based convolutions can reduce the computation and improve the performance of multi-channel 2D convolution.

In summary, our reuse algorithms can accelerate multi-channel 2D convolutions with small input channels.
