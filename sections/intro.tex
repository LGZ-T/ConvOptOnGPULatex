\section{Introduction}
Convolution is a fundamental building block for many application tasks, including image and video processing and machine learning
models \cite{Perrot2014Fine,Ma2014Optimized,Rudi2015Image,Niklaus2017Video,Krizhevsky2012ImageNet,SimonyanZ14a,HeZRS16,SzegedyLJSRAEVR15}. Convolution can be performed on multiple dimensions, but 2D and 3D convolutions are among the most frequently used
application scenarios. For examples, 2D convolution is often used in image filtering and frame
interpolation~\cite{Perrot2014Fine,Ma2014Optimized,Rudi2015Image,Niklaus2017Video}, while 3D convolution is at the core of Convolutional
Neural Networks (CNNs) \cite{Krizhevsky2012ImageNet,SimonyanZ14a,HeZRS16,SzegedyLJSRAEVR15}. While important, convolution operations are both compute- and memory-intensive \cite{cavigelli2015accelerating},
and can account for nearly 90\% of overall runtime \cite{Li2016Performance} for representative image and machine learning processing tasks.
Therefore, there is a critical need for accelerating convolution operations.


A wide range of techniques have been proposed to accelerate convolution
operations~\cite{Iandola2014Communication,vasilache2014fast,lavin2016fast,cho2017mec,Zhen2018Optimizing,Vasudevan2017Parallel,Chellapilla2006High}.
Among these, convulsions based GEMM \cite{Vasudevan2017Parallel,Chellapilla2006High}, FFT~\cite{vasilache2014fast} and
Winograd~\cite{lavin2016fast} methods are broadly adopted. All three types of convolution algorithms follow a
three-step processing pipeline. The algorithm first transforms the input and a filter (that applies on the input) into
two matrices. Next, it performs matrix multiplication on the two matrices before converting the result in the output.
Unfortunately, such as processing pipeline can incur a large number of memory transactions during the transformation
phase due to the involvement of matrix multiplications and duplicate elements of the transformed matrices. Such
overhead can be significant â€“ for example, we observe the number of memory transactions can be increased by 90\% for
the popular 3D convolution (Section\FIXME{\ref{}}). This is a serious problem for distributed memory architecture, a
commonly used design for high-end GPUs, where memory accessing is often a performance bottleneck. As a result, prior
optimization techniques leave much room for improvement when executing convolution operations on GPUs.


%%We examine the process of convolution and propose two optimization algorithms which can improve the performance of convolution by reducing
%%the number of memory transactions without incurring any additional memory storage. First, we utilize shuffle instructions to exchange elements between threads of the same warp.
%This paper aims to optimizing memory transactions for executing convolution operations on GPUs. Doing so can
%significantly improve the performance for convolution operations by reducing the number of memory access. Our technique
%is complementary to convolution optimization techniques based on GEMM, FFT and Winograd, and can be integrated with
%these techniques. To this end, this paper presents two novel optimization algorithms. Our first optimization techniques
%utilizes shuffle instructions (that supported by CUDA and OpenCL) to exchange elements among threads within the same
%GPU warp (or working group). In this way, we can avoid reloading the same elements which are shared among different
%threads. We further extend the shuffle instruction to support dynamic indexing which is not supported in the previous
%study \cite{vasilache2014fast}. We call this strategy column reuse. Second, we try to make full use of rows of the
%input since each row of the input can be used to generate multiple output elements. We call this the row reuse
%algorithm.

This paper aims to optimizing memory transactions for executing convolution operations on GPUs. Doing so can
significantly improve the performance of convolution operations by reducing the number of memory access.  To this end,
this paper presents two novel optimization techniques for operations performed on both the column and the row
directions. Our first optimization technique exploits column resue. It achieves this by utilizing shuffle instructions
(supported by CUDA and OpenCL) to exchange elements among threads within the same GPU warp (or working group). In this
way, we can avoid reloading the same elements shared among different threads. We further extend the shuffle instruction
to support dynamic indexing which is not supported in the previous study \cite{vasilache2014fast}.  Our second
optimization targets row reuse, by multiplying one input row with multiple rows of a convolutional kernel to compute
multiple output elements.

%Our technique is complementary to convolution optimization techniques based on GEMM, FFT and Winograd, and can be
%integrated with these techniques.



We apply our optimization techniques to direct convolution operations and evaluate them on an NVIDIA K40 GPU platform.
We compare with highly optimized DNN libraries, including cuDNN. Experimental results show that our approach delivers
over 2$\times$ faster performance over the best-performing competitive strategy, by significantly reduce the number of
memory transactions for convolution operations.



The main contributions of our work are as follows:
\begin{itemize}
  \item we design a column reuse algorithm. This algorithm utilizes shuffle instructions to exchange columns among different threads. Compared
      with previous study \cite{vasilache2014fast}, our column reuse algorithm can be applied in more situations.
  \item we design a row reuse algorithm. This algorithm loads each row of input once and then multiply it with multiple rows of a filter
      to calculate multiple output elements.
  \item we propose a method for transforming dynamic indexing into static indexing. This method can move arrays with dynamic indexing
      from local memory into registers, which greatly improves the performance.
  %\item we implement a 2D convolution and 3D convolutions with one and three input channels. The experiments show that our 2D and 3D convolutions are fast and memory efficient compared with state-of-the-art convolution libraries, including ArrayFire, NPP and cuDNN.
\end{itemize}
