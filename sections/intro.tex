\section{Introduction}
%\IEEEPARstart{N}{owadays}, many researchers construct the fundamental building block of their CNNs with depthwise separable convolutions initially introduced in \cite{sifre2014rigid} to reduce the computational cost of standard 2D convolutions.
%This kind of CNNs, including MobileNet \cite{Sandler_2018_CVPR,howard2019searching}, EfficientNet \cite{tan2019efficientnet} and ShuffleNet \cite{Ma_2018_ECCV}, has been widely used in image classification, object detection and  semantic segmentation.
%Depthwise separable convolution splits the computation of 2D convolution into depthwise convolution and pointwise convolution.

In recent years, deep neural networks (DNNs) have made astonishing success in solving a wide range of tasks \FIXME{\cite{}}. One of the
most successful DNN architectures is the convolutional neural network (CNN) that is widely used in tasks like image classification
\FIXME{\cite{}}, object detection \FIXME{\cite{}} and  semantic segmentation \FIXME{\cite{}}. CNN models are typically trained and run on
GPUs due to their computation requirements.


The depthwise separable convolution (\texttt{DSC}) is widely used in modern CNN models for accelerating model computation time
\FIXME{\cite{}}. This operation can process both the spatial dimensions (e.g., the width and the height of an image) and the depth
dimension (e.g., the RGB channels of an image) of an input. It achieves this by splitting a convolution kernel into two separate kernels
that perform two convolutions: a depthwise convolution and a pointwise convolution. The former applies a single convolutional filter for
each input channel, and the latter uses a $1 \times 1$ kernel to iterate through every single point of the input (e.g., the kernel has a
depth of however many channels the input image has). Compare to a classical depthwise, 3D convolution that operates on a 3D input of
$channels \times width \times height$, the DSC reduces the number of multiplication operations as well as the number of parameters needed
for the convolution filter (and hence the chance of model over-fitting) as well as computation time by having fewer arithmetic operations.
For this reason, the DSC is widely used in latency-sensitive scenarios, such as using a trained CNN on embedded devices or performing
distributed, on-device learning on resource-constrained systems~\cite{espeholt2019seed}.


A wide range of optimization techniques have been proposed to peform convolutions
\cite{li2016optimizing,Zhen2018Optimizing,enfedaque2014implementation,liu2019optimizing,winter2019adaptive,vasilache2014fast,lavin2016fast,Vasudevan2017Parallel,Chellapilla2006High}.
Among these techniques, the fast fourier transform (FFT) \cite{vasilache2014fast}, winograd (Winograd) \cite{lavin2016fast} and general
matrix multiplication (GEMM) \cite{Vasudevan2017Parallel,Chellapilla2006High} are broadly adopted. However, FFT and Winograd offer little
benefit for depthwise convolutions compared to standard 3D convolution. This is because FFT and and Winograd are designed to optimize
arithmetic computation \FIXME{\cite{}}, but not memory accesses. However, the memory access latency often dominates the execution time of
depthwise convolution \cite{cudaperformance} due to its lower arithmetic operations compared to a standard 3D convolution.  Both methods
are also ill-suite for pointwise convolution (that applies a $1 \times 1$ kernel) because FFT is designed to operated on a large filter and
Winograd works best when the filter size is $3 \times 3$.
%Both methods are not implemented for pointwise convolution in cuDNN.

%In summary, GEMM is the most suitable algorithm for depthwise and pointwise convolutions. The state-of-the-art GEMM-based method is
%implicit GEMM implemented by cuDNN \cite{ChetlurWVCTCS14}. Though the GEMM algorithm has been well optimized by cuDNN, we find two
%performance issues when applying cuDNN on depthwise separable convolutions with batch sizes of 128 or below.
%\begin{itemize}
%    \item Depthwise convolution possesses a much lower computational workload than standard 2D convolution. Therefore the memory performance is one of the most important gating factors \cite{cudaperformance}.
%    However, the implicit GEMM of cuDNN uses more global memory access operations than necessary to complete convolution (as shown in Figure \ref{fig:dwldginst} of Section \ref{sec:depconvexp}), which significantly degrades the performance of depthwise convolution.
%    \item The implementation of pointwise convolution in cuDNN can not utilize GPU efficiently (as shown in Figure \ref{fig:pwsmutil} of Section \ref{sec:pwconvexp}).
%    The reason is that cuDNN uses a fixed block size without considering the total amount of computation, which results in a small number of thread blocks when the computational workload is low.
%\end{itemize}


While GEMM is a good fit for pointwise convolution (that is also adopted by cuDNN \cite{ChetlurWVCTCS14}), the current implementation of
GEMM for CNNs can lead to poor GPU performance during model deployment. A typical GEMM implementation uses a fixed block size to distribute
work across parallel threads, without taking into consideration the amount of computation required. As we will show later in the paper,
such a strategy cannot make effective use of the GPU parallelism when the batch size (i.e., the number of samples to be processed at once)
is small (e.g., $<= 128$). The ineffective use of GPU resources leads to low GPU utilization and sub-optimal performance. This is a
particular problem for two real-life scenarios: when running a trained model for inferencing - where the model typically only takes in one
or a few samples (an hence a small batch size) - or performing on-device distributed training on an embedded device - where the number of
training samples is likely to be small due to resource constraints.



%In this work, we propose two approaches targeting memory performance and GPU utilization to improve the performance of depthwise and pointwise convolutions, respectively.
%
%To improve the memory performance of depthwise convolution, we introduce two novel optimization techniques for operations performed on columns and rows.
%The first technique exploits column reuse by utilizing shuffle instructions (supported by both CUDA and OpenCL and hence is applicable to mainstream GPUs) to exchange elements among threads within a warp (or working group).
%In this way, we can avoid reloading the same elements shared among different threads.
%We further extend the shuffle instructions to facilitate dynamic indexing, which is not supported in the previous study \cite{vasilache2014fast}.
%The second technique targets row reuse by multiplying one input row with multiple rows of a convolutional kernel (or filter) to compute multiple output elements.
%This strategy improves the data locality of elements within a row, reducing the number of memory transactions compared with that of the existing convolution processing pipeline.
%By reducing the number of memory transactions, our approach thus can improve the performance of depthwise convolution.
%
%To increase GPU utilization for pointwise convolution, we propose a dynamic block size method.
%Previous work \cite{li2019coordinated,pourghassemi2020limits} employs a heuristic method to select the best block size for general matrix multiplication (GEMM) operation.
%But there is a major restriction when applying their method to convolution operations.
%When converting the convolution operation into GEMM, the resultant matrix size is usually smaller than the expected matrix size.
%Then they try to combine multiple convolutions that can be calculated concurrently into one GEMM kernel to increase the computation workload.
%This is possible for CNNs which has multiple parallel convolutions within the same layer (e.g. inception layers in GoogleNet \cite{szegedy2015going}).
%But many other CNNs must perform convolution operations one by one to ensure correctness, which makes their method unsuitable in these CNNs.
%In our dynamic block size method, we first decrease the computational workload of each thread and create more thread blocks to saturate GPU.
%But this simple method will let each thread suffer from long global memory access latency since there are not enough arithmetic operations to hide the latency.
%To address this problem, we design a dynamic block size method that distributes input or filter channels across threads within a warp.

Our work addresses the memory latency and work distribution issues identified above. By addressing these two issues together, our approach
enables efficient DSC because it accelerate not only depthwise convolution by reducing the GPU memory access latency and also pointwise
convolution for model inferencing and small-batch-sized training.


To improve the memory performance of depthwise convolution (that operates on the width and the height directions of the input matrix), we
introduce two novel optimization techniques for operations performed on rows (i.e., widths) and columns (i.e., heights). Our approach
reduces the number of memory accesses required by reusing data. To improve column data reuse, we use the shuffle instructions (supported by
both CUDA and OpenCL and hence is applicable to mainstream GPUs) to exchange elements among threads within a GPU warp (or working group).
In this way, we can avoid reloading the same elements shared among different threads. We also apply shuffle instructions to converting
dynamic indices to static indices to assist register promotion, an optimization strategy that is not exploited in previous studies
\cite{vasilache2014fast}. To increase row data reuse, we multiple one input row with multiple rows of a convolutional kernel (or filter) to
compute multiple output elements at the same time. This strategy improves the data locality of elements within a row, reducing the number
of memory transactions compared with that of the existing convolution processing pipeline. By reducing the number of memory accesses, our
approach improves the performance of depthwise convolution.

To overcome the drawback of fixed block size work partition of a GEMM kernel for pointwise convolution, we employ a dynamic block-size
scheme. Our approach first adjusts the work assigned to each GPU so that we have a sufficient number of blocks to be distributed to GPU
threads to improve the GPU utilization. A challenge here is how to assign the right amount of work to GPU threads so that the global memory
access latency can be adequately hidden through computation. Having too little work per GPU thread will make the GPU memory access
dominates the execution while having too large work assignment will lead to low GPU utilization (as only a small number of GPU threads will
receive a block to work on). To this end, our dynamic scheme distributes input or filter channels across threads within a warp to minimize
memory latency with improved GPU parallelism. Some recent work \cite{li2019coordinated,pourghassemi2020limits} employ a heuristic method to
to maximize parallelism for GEMM. They achieve this by trying to combine multiple convolutions that can be computed concurrently into one
GEMM kernel. Such a strategy assumes multiple parallel convolutions can be performed within a single GEMM kernel. However, this strong
assumption is only valid for some special CNN structures like the inception layer in GoogleNet \cite{szegedy2015going}. As a result, these
prior methods are not applicable to the more general case of CNNs where convolution operations must be performed sequentially due to
dependence. Our dynamic work distribution strategy does not rely on this assumption and hence is more generally applicable compared to
these prior approaches.

We evaluate our approach by applying it to both depthwise and pointwise convolutions of DSC on two GPU platforms: an NVIDIA RTX 2080Ti GPU
and an embedded NVIDIA Jetson AGX Xavier GPU. We compared our approach against cuDNN, an industry strengthed DNN library that is heavily
optimized for the NVIDIA GPU architecture. Experimental results show that our approach delivers over $3\times$ and $2\times$ faster
performance than cuDNN for depthwise and pointwise convolutions, respectively. We show that our approach reduces the end-to-end training
and inference time of MobileNet \FIXME{\cite{}} when the batch size is $<=$ 128 by 14.4\% and 9.7\% on average, respectively.



This paper makes the following technical contributions:
\begin{itemize}
    \item It presents two novel algorithms for column and row reuse (Sections \ref{sec:creuse} and \ref{sec:rowreuse}) for depthwise
        convolution, improving the data locality and the memory access latency for depthwise convolution.
    \item It describes a novel method for transforming dynamic indices into static indices to assist register promotion for performance
        optimization (Section \ref{exp}).
    \item It presents a dynamic block size for pointwise convolution, increasing GPU utilization while minimizing the global memory
        access latency (Section \ref{sec:pwconv}).
\end{itemize}
