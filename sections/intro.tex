\section{Introduction\label{sec:intro}}

\IEEEPARstart{I}{n} recent years, deep neural networks (DNNs) have made astonishing success in solving a wide range of tasks \cite{zoran2020towards,yang2020gated,wang2020rdsnet,chen2020mnasfpn,zhong2020squeeze,tokunaga2019adaptive}. One of the
most successful DNN architectures is the convolutional neural network (CNN) that is widely used in tasks like image classification
\cite{zoran2020towards,yang2020gated}, object detection \cite{wang2020rdsnet,chen2020mnasfpn} and  semantic segmentation \cite{zhong2020squeeze,tokunaga2019adaptive}. CNN models are typically trained and run on
GPUs due to their computation requirements.


The depthwise separable convolution (\texttt{DSC}) is widely used in modern CNN models for accelerating model computation time
\cite{howard2019searching,tan2019efficientnet,haase2020rethinking,zhang2019depth}. This operation can process both the spatial dimensions (e.g., the width and the height of an image) and the depth
dimension (e.g., the RGB channels of an image) of an input. It achieves this by splitting a convolution kernel into two separate kernels
that perform two convolutions: a depthwise convolution and a pointwise convolution. The former applies a single convolutional filter for
each input channel, and the latter uses a $1 \times 1$ kernel to iterate through every single point of the input (e.g., the kernel has a
depth of however many channels the input image has). Compare to a classical 2D convolution that operates on a 2D input of
$channels \times height \times width$, the DSC reduces the number of multiplication operations as well as the number of parameters needed
for the convolution filter (and hence the chance of model over-fitting) as well as computation time by having fewer arithmetic operations.
For this reason, the DSC is widely used in latency-sensitive scenarios, such as using a trained CNN on embedded devices or performing
distributed, on-device learning on resource-constrained systems~\cite{espeholt2019seed}.


A wide range of optimization techniques have been proposed to perform convolutions
\cite{Zhen2018Optimizing,liu2019optimizing,winter2019adaptive,li2019autofft,yan2020optimizing,Vasudevan2017Parallel,li2019coordinated,wu2020ugemm}.
Among these techniques, the fast fourier transform (FFT) \cite{li2019autofft}, winograd (Winograd) \cite{yan2020optimizing} and general
matrix multiplication (GEMM) \cite{Vasudevan2017Parallel,li2019coordinated,wu2020ugemm} are broadly adopted. However, FFT and Winograd offer little
benefit for depthwise convolutions compared to standard 2D convolution. This is because FFT and Winograd are designed to optimize
arithmetic computation \cite{zlateski2019anatomy,yan2020optimizing}, but not memory accesses. However, the memory access latency often dominates the execution time of
depthwise convolution \cite{cudaperformance} due to its lower arithmetic operations compared to a standard 2D convolution.  Both methods
are also ill-suited for pointwise convolutions (that apply a $1 \times 1$ kernel) because FFT is designed to operate on a large filter and
Winograd works best when the filter size is $3 \times 3$.

While GEMM is a good fit for pointwise convolution (that is also adopted by cuDNN \cite{ChetlurWVCTCS14}), the current implementation of
GEMM for CNNs can lead to poor GPU performance during model deployment. A typical GEMM implementation uses a fixed tile size to distribute
work across parallel threads, without taking into consideration the amount of computation required. As we will show later in the paper,
such a strategy cannot make effective use of the GPU parallelism when the batch size (i.e., the number of samples to be processed at once)
is small (e.g., $<= 128$). The ineffective use of GPU resources leads to low GPU utilization and sub-optimal performance. This is a
particular problem for two real-life scenarios: when running a trained model for inferencing - where the model typically only takes in one
or a few samples (and hence a small batch size) - or performing on-device distributed training on an embedded device - where the number of
training samples is likely to be small due to resource constraints.

Our work addresses the memory latency and work distribution issues identified above. By addressing these two issues together, our approach
enables efficient DSC because it accelerates not only depthwise convolution by reducing the GPU memory access latency and also pointwise
convolution for model inference and small-batch-sized training.


To improve the memory performance of depthwise convolution,
%(that operates on the width and the height directions of the input matrix)
we
introduce two novel optimization techniques for operations performed on rows and columns. Our approach
reduces the number of memory accesses required by reusing data. To improve column data reuse, we use the shuffle instructions (supported by
both CUDA and OpenCL and hence is applicable to mainstream GPUs) to exchange elements among threads within a GPU warp (or working group).
In this way, we can avoid reloading the same elements shared among different threads. We also apply shuffle instructions to converting
dynamic indices to static indices to assist register promotion, an optimization strategy that is not exploited in previous studies
\cite{vasilache2014fast,chen2019versatile}. To increase row data reuse, we multiply one input row with multiple rows of a convolutional kernel (or filter) to
compute multiple output elements at the same time. This strategy improves the data locality of elements within a row, reducing the number
of memory transactions compared with that of the existing convolution processing pipeline. By reducing the number of memory accesses, our
approach improves the performance of depthwise convolution.

To overcome the drawback of fixed tile size work partition of a GEMM kernel for pointwise convolution, we employ a dynamic tile size
scheme. Our approach first adjusts the work assigned to each GPU thread so that we have a sufficient number of tiles to be distributed to GPU
threads to improve the GPU utilization. A challenge here is how to assign the right amount of work to GPU threads so that the global memory
access latency can be adequately hidden through computation. Having too little work per GPU thread will make the GPU memory access
dominates the execution while having too large work assignment will lead to low GPU utilization (as only a small number of GPU threads will
receive a tile to work on). To this end, our dynamic scheme distributes input or filter channels across threads within a warp to minimize
memory latency with improved GPU parallelism. Recent works \cite{li2019coordinated,pourghassemi2020limits} employ a heuristic method
to maximize parallelism for GEMM. They achieve this by trying to combine multiple convolutions that can be computed concurrently into one
GEMM kernel. Such a strategy assumes multiple parallel convolutions can be performed within a single GEMM kernel. However, this strong
assumption is only valid for some special CNN structures like the inception layer in GoogleNet \cite{szegedy2015going}. As a result, these
prior methods are not applicable to the more general case of CNNs where convolution operations must be performed sequentially due to
dependence. Our dynamic work distribution strategy does not rely on this assumption and hence is more generally applicable compared to
these prior approaches.

We evaluate our approach by applying it to both depthwise and pointwise convolutions with FP32 and INT8 on two GPU platforms: an NVIDIA RTX 2080Ti GPU
and an embedded NVIDIA Jetson AGX Xavier GPU. We compared our approach against cuDNN, an industry strengthened DNN library that is heavily
optimized for the NVIDIA GPU architecture. Experimental results show that our approach delivers over $3\times$ and $2\times$ faster
performance than cuDNN for depthwise and pointwise convolutions, respectively.
We show that, when the batch size is $<=$ 128, our approach averagely improves the end-to-end training performance of MobileNetV2 \cite{Sandler_2018_CVPR,howard2019searching} and EfficientNet-B0 \cite{tan2019efficientnet} by 11.5\% and 7.3\% respectively, and improves the end-to-end inference performance of MobileNetV2 and EfficentNet-B0 by 9.7\% and 11.6\% respectively.



This paper makes the following technical contributions:
\begin{itemize}
    \item It presents two novel algorithms for column and row reuse (Section \ref{sec:strategies}) for depthwise
        convolution, improving the data locality and the memory access latency for depthwise convolution.
    \item It describes a novel method for transforming dynamic indices into static indices to assist register promotion for performance
        optimization (Section \ref{sec:columnreuse}).
    \item It presents a dynamic tile size scheme for pointwise convolution, increasing GPU utilization while minimizing the global memory
        access latency (Section \ref{sec:pwconv}).
\end{itemize}

\RV{This work extends our prior work \cite{lu2020optimizing} by proposing a new dynamic tile size scheme to optimize pointwise convolution,
which is a key component of depthwise separable convolution. We also added new experiments performed on embedded devices and using 8-bit
integers for the neural networks. The new experiments demonstrate the robustness of the proposed approach, showing that it consistently
outperforms cuDNN by delivering the overall best performance.}
