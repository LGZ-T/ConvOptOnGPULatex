\section{Conclusion}
We have presented a novel approach to optimize the memory access for convolution computation on GPUs. Our approach improves the data
locality for convolutional operations performed on the row and column directions to reduce the memory access. Our techniques utilize the
common GPU shuffle operations supported by mainstream GPU programming models, including CUDA and OpenCL, and do not require hardware
modifications. We evaluate our approach by applying it to 2D, depth-wise and multi-channel 2D convolutions and evaluate it on an NVIDIA RTX
2080Ti GPU platform. For the 2D convolution, our approach doubles the performance of the state-of-the-art image processing libraries. For
the depth-wise convolution, our techniques deliver up to $4 \times$ speedups over the fastest algorithm of cuDNN. For the multi-channel 2D
convolution, proposed reuse algorithms achieve an average speedup of $1.23\times$ over the fastest algorithm of cuDNN.


%\begin{acks}                            %% acks environment is optional
%                                        %% contents suppressed with 'anonymous'
%  %% Commands \grantsponsor{<sponsorID>}{<name>}{<url>} and
%  %% \grantnum[<url>]{<sponsorID>}{<number>} should be used to
%  %% acknowledge financial support and will be used by metadata
%  %% extraction tools.
%  This material is based upon work supported by the
%  \grantsponsor{GS100000001}{National Science
%    Foundation}{http://dx.doi.org/10.13039/100000001} under Grant
%  No.~\grantnum{GS100000001}{nnnnnnn} and Grant
%  No.~\grantnum{GS100000001}{mmmmmmm}.  Any opinions, findings, and
%  conclusions or recommendations expressed in this material are those
%  of the author and do not necessarily reflect the views of the
%  National Science Foundation.
%\end{acks}
