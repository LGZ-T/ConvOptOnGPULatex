\begin{abstract}
Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for performance bottlenecks during
training and inferencing.  Existing approaches for accelerating convolution operations aim to reduce computational complexity. However,
these strategies often increase the memory footprint with extra memory access, thereby leaving an opportunity for performance optimization.

This study presents a novel approach to optimize memory access for  convolution operations, specifically targeting GPU execution.
When sliding a filter on the width dimension,
we use shuffle instructions to exchange the overlapped columns of the input and reduce the number of memory transactions. When sliding a filter on the height
dimension, we multiply each overlapped row of the input with multiple rows of a filter to calculate multiple output elements.


We apply our approach to 2D and 3D convolutions and evaluate it on two NVIDIA GPU architectures (K40 and 2080Ti). For the 2D
convolution, our approach delivers over 2x faster performance than the state-of-the-art image processing libraries. For the 3D convolution,
we obtain up to 2.5x speedups over the fastest algorithm of cuDNN.



%Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for the performance bottleneck
%during model training and inferencing. Existing approaches for accelerating convolution operations aim to reduce the computational
%complexity. However, such strategies often increase the memory footprint with extra memory accesses, leaving much room for performance
%optimization when training and running DNNs on GPUs.
%
%This paper presents a novel approach for optimizing memory accessing of convolution operations, specifically targeting GPU execution. Our
%approach leverages two optimization techniques to reduce the number of memory transactions. First, we use CUDA shuffle instructions to
%exchange overlapped columns of input when sliding a filter over an input along the width dimension. Second, we multiply each overlapped
%row of an input with multiple rows of a filter when sliding the filter along the height dimension. We apply our approach to both 2D and
%3D convolutions and evaluate it on NVIDIA Tesla K40 GPU and NVIDIA RTX 2080 Ti. For 2D convolution, our approach delivers over 2X faster
%performance than the state-of-the-art image processing libraries. For 3D convolution with one and three input channels, we obtain up to
%2.5x speedups over the fastest algorithm of cuDNN.
\end{abstract}

%% End of generated code


\keywords{Performance Optimization, Convolution, Memory Optimization, GPUs}
