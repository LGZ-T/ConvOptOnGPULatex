\begin{abstract}
The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the
computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target
accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate
for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once.
%
This paper aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by
designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations
performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively
distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our
approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit floating point (FP32) and 8-bit integer (INT8). We compared our approach against
cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over $2\times$ (up to
$3\times$) performance improvement over cuDNN. 
We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNetV2 and EfficientNet-B0 by 9.7\% and 7.3\% respectively, and reduces the end-to-end inference time of MobileNetV2 and EfficentNet-B0 by 12.2\% and 13.5\% respectively.
\end{abstract}

%% End of generated code

\begin{IEEEkeywords}
Performance Optimization, Convolution, Depthwise, Pointwise, Memory Optimization, GPU Utilization
\end{IEEEkeywords}
