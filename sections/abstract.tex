\begin{abstract}
Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for the performance bottleneck for
training and inferencing.  Existing approaches for accelerating convolution operations aim to reduce computational complexity. However,
such strategies often increase the memory footprint with extra memory accesses, leaving much room for performance optimization.

This paper presents a novel approach for optimizing memory accessing for the convolution operation, specifically targeting GPU execution.
Our approach work for convolution operations on both the width and height dimensions. When sliding a filter on the width dimension, we
shuffle instructions to exchange overlapped columns of input to \FIXME{reduce/improve???}. When sliding a filter on the height dimension,
we multiply each overlapped row of input with multiple rows of a filter to \FIXME{reduce/improve????}.


We apply our approach to both 2D and 3D convolutions and evaluate it on two NVIDIA GPU architectures (K40 and 2080Ti). For 2D
convolution, our approach delivers over 2x faster performance than the state-of-the-art image processing libraries. For 3D convolution,
we obtain up to 2.5x speedups over the fastest algorithm of cuDNN.



%Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for the performance bottleneck
%during model training and inferencing. Existing approaches for accelerating convolution operations aim to reduce the computational
%complexity. However, such strategies often increase the memory footprint with extra memory accesses, leaving much room for performance
%optimization when training and running DNNs on GPUs.
%
%This paper presents a novel approach for optimizing memory accessing of convolution operations, specifically targeting GPU execution. Our
%approach leverages two optimization techniques to reduce the number of memory transactions. First, we use CUDA shuffle instructions to
%exchange overlapped columns of input when sliding a filter over an input along the width dimension. Second, we multiply each overlapped
%row of an input with multiple rows of a filter when sliding the filter along the height dimension. We apply our approach to both 2D and
%3D convolutions and evaluate it on NVIDIA Tesla K40 GPU and NVIDIA RTX 2080 Ti. For 2D convolution, our approach delivers over 2X faster
%performance than the state-of-the-art image processing libraries. For 3D convolution with one and three input channels, we obtain up to
%2.5x speedups over the fastest algorithm of cuDNN.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML}
<ccs2012> <concept> <concept_id>10011007.10011006.10011008</concept_id>
<concept_desc>Software and its engineering~General programming languages</concept_desc>
<concept_significance>500</concept_significance> </concept>

<concept>
<concept_id>10003456.10003457.10003521.10003525</concept_id>
<concept_desc>Social and professional topics~History of programming
languages</concept_desc>
<concept_significance>300</concept_significance> </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~General programming languages} \ccsdesc[300]{Social and professional topics~History of
programming languages}
%% End of generated code


\keywords{Performance Optimization, Convolution, Memory Optimization, GPUs}
