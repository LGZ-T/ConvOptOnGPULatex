\begin{abstract}
Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for performance bottlenecks during
training and inferencing.  Existing approaches for accelerating convolution operations aim to reduce computational complexity. However,
these strategies often increase the memory footprint with extra memory access, thereby leaving much room for performance improvement.
%
This paper presents a novel approach to optimize memory access for convolution operations, specifically targeting GPU execution. Our
approach leverages two optimization techniques to reduce the number of memory operations for convolution operations performed on the
width and the hight dimensions. For convolution computations on the width dimension, we exploit shuffle instructions to exchange the
overlapped columns of the input for reducing the number of memory transactions. For convolution operations on the height dimension, we
multiply each overlapped row of the input with multiple rows of a filter to compute multiple output elements to improve the data locality
of row elements. Our approach is simple yet effective and can work on existing CUDA and OpenCL standards without hardware modification.
%
We apply our approach to 2D and 3D convolutions and evaluate it on two NVIDIA GPU architectures (K40 and 2080Ti). For the 2D
convolution, our approach delivers over 2x faster performance than the state-of-the-art image processing libraries. For the 3D convolution,
we obtain up to 2.5x speedups over the fastest algorithm of cuDNN.



%Convolution computation is a common operation in deep neural networks (DNNs) and is often responsible for the performance bottleneck
%during model training and inferencing. Existing approaches for accelerating convolution operations aim to reduce the computational
%complexity. However, such strategies often increase the memory footprint with extra memory accesses, leaving much room for performance
%optimization when training and running DNNs on GPUs.
%
%This paper presents a novel approach for optimizing memory accessing of convolution operations, specifically targeting GPU execution. Our
%approach leverages two optimization techniques to reduce the number of memory transactions. First, we use CUDA shuffle instructions to
%exchange overlapped columns of input when sliding a filter over an input along the width dimension. Second, we multiply each overlapped
%row of an input with multiple rows of a filter when sliding the filter along the height dimension. We apply our approach to both 2D and
%3D convolutions and evaluate it on NVIDIA Tesla K40 GPU and NVIDIA RTX 2080 Ti. For 2D convolution, our approach delivers over 2X faster
%performance than the state-of-the-art image processing libraries. For 3D convolution with one and three input channels, we obtain up to
%2.5x speedups over the fastest algorithm of cuDNN.
\end{abstract}

%% End of generated code


\keywords{Performance Optimization, Convolution, Memory Optimization, GPUs}
